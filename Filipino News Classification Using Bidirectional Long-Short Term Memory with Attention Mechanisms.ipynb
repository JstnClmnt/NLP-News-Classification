{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing, Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = dirName+\"/\"+entry\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1.txt\n",
      "Reading 2.txt\n",
      "Reading 3.txt\n",
      "Reading 4.txt\n",
      "Reading 5.txt\n",
      "Reading 7.txt\n",
      "Reading 8.txt\n",
      "Reading 9.txt\n",
      "Reading 10.txt\n",
      "Reading 11.txt\n",
      "Reading 12.txt\n",
      "Reading 15.txt\n",
      "Reading 16.txt\n",
      "Reading 19.txt\n",
      "Reading 20.txt\n",
      "Reading 21.txt\n",
      "Reading 22.txt\n",
      "Reading 23.txt\n",
      "Reading 24.txt\n",
      "Reading 25.txt\n",
      "Reading 26.txt\n",
      "Reading 27.txt\n",
      "Reading 28.txt\n",
      "Reading 29.txt\n",
      "Reading 30.txt\n",
      "Reading 31.txt\n",
      "Reading 32.txt\n",
      "Reading 33.txt\n",
      "Reading 34.txt\n",
      "Reading 35.txt\n",
      "Reading 36.txt\n",
      "Reading 37.txt\n",
      "Reading 38.txt\n",
      "Reading 39.txt\n",
      "Reading 40.txt\n",
      "Reading 41.txt\n",
      "Reading 42.txt\n",
      "Reading 43.txt\n",
      "Reading 44.txt\n",
      "Reading 45.txt\n",
      "Reading 46.txt\n",
      "Reading 47.txt\n",
      "Reading 48.txt\n",
      "Reading 49.txt\n",
      "Reading 50.txt\n",
      "Reading 51.txt\n",
      "Reading 52.txt\n",
      "Reading 53.txt\n",
      "Reading 54.txt\n",
      "Reading 55.txt\n",
      "Reading 56.txt\n",
      "Reading 57.txt\n",
      "Reading 58.txt\n",
      "Reading 59.txt\n",
      "Reading 60.txt\n",
      "Reading 61.txt\n",
      "Reading 62.txt\n",
      "Reading 63.txt\n",
      "Reading 64.txt\n",
      "Reading 65.txt\n",
      "Reading 66.txt\n",
      "Reading 67.txt\n",
      "Reading 68.txt\n",
      "Reading 69.txt\n",
      "Reading 70.txt\n",
      "Reading 71.txt\n",
      "Reading 72.txt\n",
      "Reading 73.txt\n",
      "Reading 74.txt\n",
      "Reading 75.txt\n",
      "Reading 76.txt\n",
      "Reading 77.txt\n",
      "Reading 78.txt\n",
      "Reading 79.txt\n",
      "Reading 80.txt\n",
      "Reading 81.txt\n",
      "Reading 82.txt\n",
      "Reading 83.txt\n",
      "Reading 84.txt\n",
      "Reading 85.txt\n",
      "Reading 86.txt\n",
      "Reading 87.txt\n",
      "Reading 88.txt\n",
      "Reading 89.txt\n",
      "Reading 90.txt\n",
      "Reading 91.txt\n",
      "Reading 92.txt\n",
      "Reading 93.txt\n",
      "Reading 94.txt\n",
      "Reading 95.txt\n",
      "Reading 96.txt\n",
      "Reading 97.txt\n",
      "Reading 98.txt\n",
      "Reading 99.txt\n",
      "Reading 100.txt\n",
      "Reading 101.txt\n",
      "Reading 102.txt\n",
      "Reading 103.txt\n",
      "Reading 104.txt\n",
      "Reading 105.txt\n",
      "Reading 106.txt\n",
      "Reading 107.txt\n",
      "Reading 108.txt\n",
      "Reading 109.txt\n",
      "Reading 110.txt\n",
      "Reading 111.txt\n",
      "Reading 112.txt\n",
      "Reading 113.txt\n",
      "Reading 114.txt\n",
      "Reading 115.txt\n",
      "Reading 116.txt\n",
      "Reading 117.txt\n",
      "Reading 118.txt\n",
      "Reading 119.txt\n",
      "Reading 120.txt\n",
      "Reading 121.txt\n",
      "Reading 122.txt\n",
      "Reading 123.txt\n",
      "Reading 124.txt\n",
      "Reading 125.txt\n",
      "Reading 126.txt\n",
      "Reading 127.txt\n",
      "Reading 128.txt\n",
      "Reading 129.txt\n",
      "Reading 130.txt\n",
      "Reading 131.txt\n",
      "Reading 132.txt\n",
      "Reading 133.txt\n",
      "Reading 134.txt\n",
      "Reading 135.txt\n",
      "Reading 136.txt\n",
      "Reading 137.txt\n",
      "Reading 138.txt\n",
      "Reading 139.txt\n",
      "Reading 140.txt\n",
      "Reading 141.txt\n",
      "Reading 142.txt\n",
      "Reading 143.txt\n",
      "Reading 144.txt\n",
      "Reading 145.txt\n",
      "Reading 146.txt\n",
      "Reading 147.txt\n",
      "Reading 148.txt\n",
      "Reading 149.txt\n",
      "Reading 150.txt\n",
      "Reading 151.txt\n",
      "Reading 152.txt\n",
      "Reading 153.txt\n",
      "Reading 154.txt\n",
      "Reading 155.txt\n",
      "Reading 156.txt\n",
      "Reading 157.txt\n",
      "Reading 158.txt\n",
      "Reading 159.txt\n",
      "Reading 160.txt\n",
      "Reading 161.txt\n",
      "Reading 162.txt\n",
      "Reading 163.txt\n",
      "Reading 164.txt\n",
      "Reading 165.txt\n",
      "Reading 166.txt\n",
      "Reading 167.txt\n",
      "Reading 168.txt\n",
      "Reading 169.txt\n",
      "Reading 170.txt\n",
      "Reading 171.txt\n",
      "Reading 172.txt\n",
      "Reading 173.txt\n",
      "Reading 174.txt\n",
      "Reading 175.txt\n",
      "Reading 176.txt\n",
      "Reading 177.txt\n",
      "Reading 178.txt\n",
      "Reading 179.txt\n",
      "Reading 180.txt\n",
      "Reading 181.txt\n",
      "Reading 182.txt\n",
      "Reading 183.txt\n",
      "Reading 184.txt\n",
      "Reading 185.txt\n",
      "Reading 186.txt\n",
      "Reading 187.txt\n",
      "Reading 188.txt\n",
      "Reading 189.txt\n",
      "Reading 190.txt\n",
      "Reading 191.txt\n",
      "Reading 192.txt\n",
      "Reading 193.txt\n",
      "Reading 194.txt\n",
      "Reading 195.txt\n",
      "Reading 196.txt\n",
      "Reading 197.txt\n",
      "Reading 198.txt\n",
      "Reading 199.txt\n",
      "Reading 200.txt\n",
      "Reading 201.txt\n",
      "Reading 202.txt\n",
      "Reading 203.txt\n",
      "Reading 204.txt\n",
      "Reading 205.txt\n",
      "Reading 206.txt\n",
      "Reading 207.txt\n",
      "Reading 208.txt\n",
      "Reading 209.txt\n",
      "Reading 210.txt\n",
      "Reading 211.txt\n",
      "Reading 212.txt\n",
      "Reading 213.txt\n",
      "Reading 214.txt\n",
      "Reading 215.txt\n",
      "Reading 216.txt\n",
      "Reading 217.txt\n",
      "Reading 218.txt\n",
      "Reading 219.txt\n",
      "Reading 220.txt\n",
      "Reading 221.txt\n",
      "Reading 222.txt\n",
      "Reading 223.txt\n",
      "Reading 224.txt\n",
      "Reading 225.txt\n",
      "Reading 226.txt\n",
      "Reading 227.txt\n",
      "Reading 228.txt\n",
      "Reading 229.txt\n",
      "Reading 230.txt\n",
      "Reading 231.txt\n",
      "Reading 232.txt\n",
      "Reading 233.txt\n",
      "Reading 234.txt\n",
      "Reading 235.txt\n",
      "Reading 236.txt\n",
      "Reading 237.txt\n",
      "Reading 238.txt\n",
      "Reading 239.txt\n",
      "Reading 240.txt\n",
      "Reading 241.txt\n",
      "Reading 242.txt\n",
      "Reading 243.txt\n",
      "Reading 244.txt\n",
      "Reading 245.txt\n",
      "Reading 246.txt\n",
      "Reading 247.txt\n",
      "Reading 248.txt\n",
      "Reading 249.txt\n",
      "Reading 251.txt\n",
      "Reading 252.txt\n",
      "Reading 253.txt\n",
      "Reading 254.txt\n",
      "Reading 255.txt\n",
      "Reading 256.txt\n",
      "Reading 257.txt\n",
      "Reading 258.txt\n",
      "Reading 259.txt\n",
      "Reading 260.txt\n",
      "Reading 261.txt\n",
      "Reading 262.txt\n",
      "Reading 263.txt\n",
      "Reading 264.txt\n",
      "Reading 265.txt\n",
      "Reading 266.txt\n",
      "Reading 267.txt\n",
      "Reading 268.txt\n",
      "Reading 269.txt\n",
      "Reading 270.txt\n",
      "Reading 271.txt\n",
      "Reading 272.txt\n",
      "Reading 273.txt\n",
      "Reading 274.txt\n",
      "Reading 275.txt\n",
      "Reading 276.txt\n",
      "Reading 277.txt\n",
      "Reading 278.txt\n",
      "Reading 279.txt\n",
      "Reading 280.txt\n",
      "Reading 281.txt\n",
      "Reading 282.txt\n",
      "Reading 283.txt\n",
      "Reading 284.txt\n",
      "Reading 285.txt\n",
      "Reading 286.txt\n",
      "Reading 287.txt\n",
      "Reading 288.txt\n",
      "Reading 289.txt\n",
      "Reading 290.txt\n",
      "Reading 291.txt\n",
      "Reading 292.txt\n",
      "Reading 293.txt\n",
      "Reading 294.txt\n",
      "Reading 295.txt\n",
      "Reading 296.txt\n",
      "Reading 297.txt\n",
      "Reading 298.txt\n",
      "Reading 299.txt\n",
      "Reading 300.txt\n",
      "Reading 301.txt\n",
      "Reading 302.txt\n",
      "Reading 303.txt\n",
      "Reading 304.txt\n",
      "Reading 307.txt\n",
      "Reading 308.txt\n",
      "Reading 309.txt\n",
      "Reading 310.txt\n",
      "Reading 318.txt\n",
      "Reading 319.txt\n",
      "Reading 326.txt\n",
      "Reading 328.txt\n",
      "Reading 330.txt\n",
      "Reading 332.txt\n",
      "Reading 333.txt\n",
      "Reading 334.txt\n",
      "Reading 335.txt\n",
      "Reading 336.txt\n",
      "Reading 337.txt\n",
      "Reading 338.txt\n",
      "Reading 339.txt\n",
      "Reading 340.txt\n",
      "Reading 341.txt\n",
      "Reading 343.txt\n",
      "Reading 344.txt\n",
      "Reading 345.txt\n",
      "Reading 346.txt\n",
      "Reading 347.txt\n",
      "Reading 348.txt\n",
      "Reading 350.txt\n",
      "Reading 351.txt\n",
      "Reading 352.txt\n",
      "Reading 353.txt\n",
      "Reading 354.txt\n",
      "Reading 355.txt\n",
      "Reading 356.txt\n",
      "Reading 357.txt\n",
      "Reading 358.txt\n",
      "Reading 359.txt\n",
      "Reading 360.txt\n",
      "Reading 361.txt\n",
      "Reading 362.txt\n",
      "Reading 363.txt\n",
      "Reading 364.txt\n",
      "Reading 365.txt\n",
      "Reading 366.txt\n",
      "Reading 367.txt\n",
      "Reading 368.txt\n",
      "Reading 369.txt\n",
      "Reading 370.txt\n",
      "Reading 371.txt\n",
      "Reading 372.txt\n",
      "Reading 373.txt\n",
      "Reading 374.txt\n",
      "Reading 375.txt\n",
      "Reading 376.txt\n",
      "Reading 377.txt\n",
      "Reading 378.txt\n",
      "Reading 379.txt\n",
      "Reading 380.txt\n",
      "Reading 381.txt\n",
      "Reading 382.txt\n",
      "Reading 383.txt\n",
      "Reading 384.txt\n",
      "Reading 385.txt\n",
      "Reading 386.txt\n",
      "Reading 387.txt\n",
      "Reading 388.txt\n",
      "Reading 389.txt\n",
      "Reading 390.txt\n",
      "Reading 391.txt\n",
      "Reading 392.txt\n",
      "Reading 393.txt\n",
      "Reading 394.txt\n",
      "Reading 395.txt\n",
      "Reading 396.txt\n",
      "Reading 397.txt\n",
      "Reading 398.txt\n",
      "Reading 399.txt\n",
      "Reading 400.txt\n",
      "Reading 401.txt\n",
      "Reading 402.txt\n",
      "Reading 403.txt\n",
      "Reading 404.txt\n",
      "Reading 405.txt\n",
      "Reading 406.txt\n",
      "Reading 407.txt\n",
      "Reading 408.txt\n",
      "Reading 409.txt\n",
      "Reading 410.txt\n",
      "Reading 411.txt\n",
      "Reading 412.txt\n",
      "Reading 413.txt\n",
      "Reading 414.txt\n",
      "Reading 415.txt\n",
      "Reading 416.txt\n",
      "Reading 417.txt\n",
      "Reading 418.txt\n",
      "Reading 419.txt\n",
      "Reading 420.txt\n",
      "Reading 421.txt\n",
      "Reading 422.txt\n",
      "Reading 423.txt\n",
      "Reading 424.txt\n",
      "Reading 425.txt\n",
      "Reading 426.txt\n",
      "Reading 427.txt\n",
      "Reading 428.txt\n",
      "Reading 429.txt\n",
      "Reading 430.txt\n",
      "Reading 431.txt\n",
      "Reading 432.txt\n",
      "Reading 433.txt\n",
      "Reading 434.txt\n",
      "Reading 435.txt\n",
      "Reading 436.txt\n",
      "Reading 437.txt\n",
      "Reading 438.txt\n",
      "Reading 439.txt\n",
      "Reading 440.txt\n",
      "Reading 441.txt\n",
      "Reading 442.txt\n",
      "Reading 443.txt\n",
      "Reading 444.txt\n",
      "Reading 445.txt\n",
      "Reading 446.txt\n",
      "Reading 447.txt\n",
      "Reading 448.txt\n",
      "Reading 449.txt\n",
      "Reading 450.txt\n",
      "Reading 451.txt\n",
      "Reading 452.txt\n",
      "Reading 453.txt\n",
      "Reading 454.txt\n",
      "Reading 455.txt\n",
      "Reading 456.txt\n",
      "Reading 457.txt\n",
      "Reading 458.txt\n",
      "Reading 459.txt\n",
      "Reading 460.txt\n",
      "Reading 461.txt\n",
      "Reading 462.txt\n",
      "Reading 463.txt\n",
      "Reading 464.txt\n",
      "Reading 465.txt\n",
      "Reading 466.txt\n",
      "Reading 467.txt\n",
      "Reading 468.txt\n",
      "Reading 469.txt\n",
      "Reading 470.txt\n",
      "Reading 471.txt\n",
      "Reading 472.txt\n",
      "Reading 473.txt\n",
      "Reading 474.txt\n",
      "Reading 475.txt\n",
      "Reading 476.txt\n",
      "Reading 477.txt\n",
      "Reading 478.txt\n",
      "Reading 479.txt\n",
      "Reading 480.txt\n",
      "Reading 481.txt\n",
      "Reading 482.txt\n",
      "Reading 483.txt\n",
      "Reading 484.txt\n",
      "Reading 485.txt\n",
      "Reading 486.txt\n",
      "Reading 487.txt\n",
      "Reading 488.txt\n",
      "Reading 489.txt\n",
      "Reading 490.txt\n",
      "Reading 491.txt\n",
      "Reading 492.txt\n",
      "Reading 493.txt\n",
      "Reading 494.txt\n",
      "Reading 495.txt\n",
      "Reading 496.txt\n",
      "Reading 497.txt\n",
      "Reading 498.txt\n",
      "Reading 499.txt\n",
      "Reading 500.txt\n",
      "Reading 501.txt\n",
      "Reading 502.txt\n",
      "Reading 503.txt\n",
      "Reading 504.txt\n",
      "Reading 505.txt\n",
      "Reading 506.txt\n",
      "Reading 507.txt\n",
      "Reading 508.txt\n",
      "Reading 509.txt\n",
      "Reading 510.txt\n",
      "Reading 511.txt\n",
      "Reading 512.txt\n",
      "Reading 513.txt\n",
      "Reading 514.txt\n",
      "Reading 515.txt\n",
      "Reading 516.txt\n",
      "Reading 517.txt\n",
      "Reading 518.txt\n",
      "Reading 519.txt\n",
      "Reading 520.txt\n",
      "Reading 521.txt\n",
      "Reading 522.txt\n",
      "Reading 523.txt\n",
      "Reading 524.txt\n",
      "Reading 548.txt\n",
      "Reading 549.txt\n",
      "Reading 550.txt\n",
      "Reading 551.txt\n",
      "Reading 552.txt\n",
      "Reading 553.txt\n",
      "Reading 554.txt\n",
      "Reading 555.txt\n",
      "Reading 556.txt\n",
      "Reading 557.txt\n",
      "Reading 558.txt\n",
      "Reading 559.txt\n",
      "Reading 560.txt\n",
      "Reading 561.txt\n",
      "Reading 562.txt\n",
      "Reading 563.txt\n",
      "Reading 564.txt\n",
      "Reading 565.txt\n",
      "Reading 566.txt\n",
      "Reading 567.txt\n",
      "Reading 568.txt\n",
      "Reading 569.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 570.txt\n",
      "Reading 571.txt\n",
      "Reading 572.txt\n",
      "Reading 573.txt\n",
      "Reading 574.txt\n",
      "Reading 575.txt\n",
      "Reading 576.txt\n",
      "Reading 577.txt\n",
      "Reading 578.txt\n",
      "Reading 579.txt\n",
      "Reading 580.txt\n",
      "Reading 581.txt\n",
      "Reading 582.txt\n",
      "Reading 583.txt\n",
      "Reading 584.txt\n",
      "Reading 585.txt\n",
      "Reading 586.txt\n",
      "Reading 587.txt\n",
      "Reading 588.txt\n",
      "Reading 589.txt\n",
      "Reading 590.txt\n",
      "Reading 591.txt\n",
      "Reading 592.txt\n",
      "Reading 593.txt\n",
      "Reading 594.txt\n",
      "Reading 595.txt\n",
      "Reading 596.txt\n",
      "Reading 597.txt\n",
      "Reading 598.txt\n",
      "Reading 599.txt\n",
      "Reading 600.txt\n",
      "Reading 601.txt\n",
      "Reading 602.txt\n",
      "Reading 603.txt\n",
      "Reading 604.txt\n",
      "Reading 605.txt\n",
      "Reading 606.txt\n",
      "Reading 607.txt\n",
      "Reading 608.txt\n",
      "Reading 609.txt\n",
      "Reading 610.txt\n",
      "Reading 611.txt\n",
      "Reading 612.txt\n",
      "Reading 613.txt\n",
      "Reading 614.txt\n",
      "Reading 615.txt\n",
      "Reading 616.txt\n",
      "Reading 617.txt\n",
      "Reading 618.txt\n",
      "Reading 619.txt\n",
      "Reading 620.txt\n",
      "Reading 621.txt\n",
      "Reading 622.txt\n",
      "Reading 623.txt\n",
      "Reading 624.txt\n",
      "Reading 625.txt\n",
      "Reading 626.txt\n",
      "Reading 627.txt\n",
      "Reading 628.txt\n",
      "Reading 629.txt\n",
      "Reading 630.txt\n",
      "Reading 631.txt\n",
      "Reading 632.txt\n",
      "Reading 633.txt\n",
      "Reading 634.txt\n",
      "Reading 635.txt\n",
      "Reading 636.txt\n",
      "Reading 637.txt\n",
      "Reading 638.txt\n",
      "Reading 639.txt\n",
      "Reading 640.txt\n",
      "Reading 641.txt\n",
      "Reading 642.txt\n",
      "Reading 643.txt\n",
      "Reading 644.txt\n",
      "Reading 645.txt\n",
      "Reading 646.txt\n",
      "Reading 647.txt\n",
      "Reading 648.txt\n",
      "Reading 649.txt\n",
      "Reading 650.txt\n",
      "Reading 651.txt\n",
      "Reading 652.txt\n",
      "Reading 653.txt\n",
      "Reading 654.txt\n",
      "Reading 655.txt\n",
      "Reading 656.txt\n",
      "Reading 657.txt\n",
      "Reading 658.txt\n",
      "Reading 659.txt\n",
      "Reading 660.txt\n",
      "Reading 661.txt\n",
      "Reading 662.txt\n",
      "Reading 663.txt\n",
      "Reading 664.txt\n",
      "Reading 665.txt\n",
      "Reading 666.txt\n",
      "Reading 667.txt\n",
      "Reading 668.txt\n",
      "Reading 669.txt\n",
      "Reading 670.txt\n",
      "Reading 671.txt\n",
      "Reading 672.txt\n",
      "Reading 673.txt\n",
      "Reading 674.txt\n",
      "Reading 675.txt\n",
      "Reading 676.txt\n",
      "Reading 677.txt\n",
      "Reading 678.txt\n",
      "Reading 679.txt\n",
      "Reading 680.txt\n",
      "Reading 681.txt\n",
      "Reading 682.txt\n",
      "Reading 683.txt\n",
      "Reading 684.txt\n",
      "Reading 685.txt\n",
      "Reading 686.txt\n",
      "Reading 687.txt\n",
      "Reading 688.txt\n",
      "Reading 689.txt\n",
      "Reading 690.txt\n",
      "Reading 691.txt\n",
      "Reading 692.txt\n",
      "Reading 693.txt\n",
      "Reading 694.txt\n",
      "Reading 695.txt\n",
      "Reading 696.txt\n",
      "Reading 697.txt\n",
      "Reading 698.txt\n",
      "Reading 699.txt\n",
      "Reading 700.txt\n",
      "Reading 701.txt\n",
      "Reading 702.txt\n",
      "Reading 703.txt\n",
      "Reading 704.txt\n",
      "Reading 705.txt\n",
      "Reading 706.txt\n",
      "Reading 707.txt\n",
      "Reading 708.txt\n",
      "Reading 709.txt\n",
      "Reading 710.txt\n",
      "Reading 711.txt\n",
      "Reading 712.txt\n",
      "Reading 713.txt\n",
      "Reading 714.txt\n",
      "Reading 715.txt\n",
      "Reading 716.txt\n",
      "Reading 717.txt\n",
      "Reading 718.txt\n",
      "Reading 719.txt\n",
      "Reading 720.txt\n",
      "Reading 721.txt\n",
      "Reading 722.txt\n",
      "Reading 723.txt\n",
      "Reading 724.txt\n",
      "Reading 725.txt\n",
      "Reading 726.txt\n",
      "Reading 727.txt\n",
      "Reading 728.txt\n",
      "Reading 729.txt\n",
      "Reading 730.txt\n",
      "Reading 731.txt\n",
      "Reading 732.txt\n",
      "Reading 733.txt\n",
      "Reading 734.txt\n",
      "Reading 735.txt\n",
      "Reading 736.txt\n",
      "Reading 737.txt\n",
      "Reading 738.txt\n",
      "Reading 739.txt\n",
      "Reading 740.txt\n",
      "Reading 741.txt\n",
      "Reading 742.txt\n",
      "Reading 743.txt\n",
      "Reading 744.txt\n",
      "Reading 745.txt\n",
      "Reading 746.txt\n",
      "Reading 747.txt\n",
      "Reading 748.txt\n",
      "Reading 749.txt\n",
      "Reading 750.txt\n",
      "Reading 751.txt\n",
      "Reading 752.txt\n",
      "Reading 753.txt\n",
      "Reading 754.txt\n",
      "Reading 755.txt\n",
      "Reading 759.txt\n",
      "Reading 761.txt\n",
      "Reading 767.txt\n",
      "Reading 772.txt\n",
      "Reading 774.txt\n",
      "Reading 775.txt\n",
      "Reading 776.txt\n",
      "Reading 777.txt\n",
      "Reading 778.txt\n",
      "Reading 779.txt\n",
      "Reading 780.txt\n",
      "Reading 781.txt\n",
      "Reading 782.txt\n",
      "Reading 783.txt\n",
      "Reading 784.txt\n",
      "Reading 785.txt\n",
      "Reading 786.txt\n",
      "Reading 787.txt\n",
      "Reading 788.txt\n",
      "Reading 789.txt\n",
      "Reading 790.txt\n",
      "Reading 791.txt\n",
      "Reading 792.txt\n",
      "Reading 793.txt\n",
      "Reading 794.txt\n",
      "Reading 795.txt\n",
      "Reading 796.txt\n",
      "Reading 797.txt\n",
      "Reading 798.txt\n",
      "Reading 799.txt\n",
      "Reading 800.txt\n",
      "Reading 801.txt\n",
      "Reading 802.txt\n",
      "Reading 803.txt\n",
      "Reading 804.txt\n",
      "Reading 805.txt\n",
      "Reading 806.txt\n",
      "Reading 807.txt\n",
      "Reading 808.txt\n",
      "Reading 809.txt\n",
      "Reading 810.txt\n",
      "Reading 811.txt\n",
      "Reading 812.txt\n",
      "Reading 813.txt\n",
      "Reading 814.txt\n",
      "Reading 815.txt\n",
      "Reading 816.txt\n",
      "Reading 817.txt\n",
      "Reading 818.txt\n",
      "Reading 819.txt\n",
      "Reading 820.txt\n",
      "Reading 821.txt\n",
      "Reading 822.txt\n",
      "Reading 823.txt\n",
      "Reading 824.txt\n",
      "Reading 825.txt\n",
      "Reading 826.txt\n",
      "Reading 827.txt\n",
      "Reading 828.txt\n",
      "Reading 829.txt\n",
      "Reading 830.txt\n",
      "Reading 831.txt\n",
      "Reading 832.txt\n",
      "Reading 833.txt\n",
      "Reading 834.txt\n",
      "Reading 835.txt\n",
      "Reading 836.txt\n",
      "Reading 837.txt\n",
      "Reading 838.txt\n",
      "Reading 839.txt\n",
      "Reading 840.txt\n",
      "Reading 841.txt\n",
      "Reading 842.txt\n",
      "Reading 843.txt\n",
      "Reading 844.txt\n",
      "Reading 845.txt\n",
      "Reading 846.txt\n",
      "Reading 847.txt\n",
      "Reading 848.txt\n",
      "Reading 849.txt\n",
      "Reading 850.txt\n",
      "Reading 851.txt\n",
      "Reading 852.txt\n",
      "Reading 853.txt\n",
      "Reading 854.txt\n",
      "Reading 855.txt\n",
      "Reading 856.txt\n",
      "Reading 857.txt\n",
      "Reading 858.txt\n",
      "Reading 859.txt\n",
      "Reading 860.txt\n",
      "Reading 862.txt\n",
      "Reading 863.txt\n",
      "Reading 864.txt\n",
      "Reading 865.txt\n",
      "Reading 866.txt\n",
      "Reading 867.txt\n",
      "Reading 868.txt\n",
      "Reading 869.txt\n",
      "Reading 870.txt\n",
      "Reading 871.txt\n",
      "Reading 872.txt\n",
      "Reading 873.txt\n",
      "Reading 874.txt\n",
      "Reading 875.txt\n",
      "Reading 876.txt\n",
      "Reading 877.txt\n",
      "Reading 878.txt\n",
      "Reading 879.txt\n",
      "Reading 880.txt\n",
      "Reading 881.txt\n",
      "Reading 882.txt\n",
      "Reading 883.txt\n",
      "Reading 884.txt\n",
      "Reading 885.txt\n",
      "Reading 886.txt\n",
      "Reading 887.txt\n",
      "Reading 888.txt\n",
      "Reading 889.txt\n",
      "Reading 890.txt\n",
      "Reading 891.txt\n",
      "Reading 892.txt\n",
      "Reading 893.txt\n",
      "Reading 894.txt\n",
      "Reading 895.txt\n",
      "Reading 896.txt\n",
      "Reading 897.txt\n",
      "Reading 898.txt\n",
      "Reading 899.txt\n",
      "Reading 900.txt\n",
      "Reading 901.txt\n",
      "Reading 902.txt\n",
      "Reading 903.txt\n",
      "Reading 904.txt\n",
      "Reading 905.txt\n",
      "Reading 906.txt\n",
      "Reading 907.txt\n",
      "Reading 908.txt\n",
      "Reading 909.txt\n",
      "Reading 910.txt\n",
      "Reading 911.txt\n",
      "Reading 912.txt\n",
      "Reading 913.txt\n",
      "Reading 914.txt\n",
      "Reading 915.txt\n",
      "Reading 916.txt\n",
      "Reading 917.txt\n",
      "Reading 918.txt\n",
      "Reading 919.txt\n",
      "Reading 920.txt\n",
      "Reading 921.txt\n",
      "Reading 922.txt\n",
      "Reading 923.txt\n",
      "Reading 924.txt\n",
      "Reading 925.txt\n",
      "Reading 926.txt\n",
      "Reading 927.txt\n",
      "Reading 928.txt\n",
      "Reading 929.txt\n",
      "Reading 930.txt\n",
      "Reading 931.txt\n",
      "Reading 932.txt\n",
      "Reading 933.txt\n",
      "Reading 934.txt\n",
      "Reading 935.txt\n",
      "Reading 936.txt\n",
      "Reading 937.txt\n",
      "Reading 938.txt\n",
      "Reading 939.txt\n",
      "Reading 940.txt\n",
      "Reading 941.txt\n",
      "Reading 942.txt\n",
      "Reading 943.txt\n",
      "Reading 944.txt\n",
      "Reading 945.txt\n",
      "Reading 946.txt\n",
      "Reading 947.txt\n",
      "Reading 948.txt\n",
      "Reading 949.txt\n",
      "Reading 950.txt\n",
      "Reading 951.txt\n",
      "Reading 952.txt\n",
      "Reading 953.txt\n",
      "Reading 954.txt\n",
      "Reading 955.txt\n",
      "Reading 956.txt\n",
      "Reading 957.txt\n",
      "Reading 958.txt\n",
      "Reading 959.txt\n",
      "Reading 960.txt\n",
      "Reading 961.txt\n",
      "Reading 962.txt\n",
      "Reading 963.txt\n",
      "Reading 964.txt\n",
      "Reading 965.txt\n",
      "Reading 966.txt\n",
      "Reading 967.txt\n",
      "Reading 968.txt\n",
      "Reading 969.txt\n",
      "Reading 970.txt\n",
      "Reading 971.txt\n",
      "Reading 972.txt\n",
      "Reading 973.txt\n",
      "Reading 974.txt\n",
      "Reading 975.txt\n",
      "Reading 976.txt\n",
      "Reading 977.txt\n",
      "Reading 978.txt\n",
      "Reading 979.txt\n",
      "Reading 980.txt\n",
      "Reading 981.txt\n",
      "Reading 982.txt\n",
      "Reading 983.txt\n",
      "Reading 984.txt\n",
      "Reading 985.txt\n",
      "Reading 986.txt\n",
      "Reading 987.txt\n",
      "Reading 988.txt\n",
      "Reading 989.txt\n",
      "Reading 990.txt\n",
      "Reading 991.txt\n",
      "Reading 992.txt\n",
      "Reading 993.txt\n",
      "Reading 994.txt\n",
      "Reading 995.txt\n",
      "Reading 996.txt\n",
      "Reading 998.txt\n",
      "Reading 1001.txt\n",
      "Reading 1002.txt\n",
      "Reading 1003.txt\n",
      "Reading 1004.txt\n",
      "Reading 1005.txt\n",
      "Reading 1006.txt\n",
      "Reading 1007.txt\n",
      "Reading 1008.txt\n",
      "Reading 1009.txt\n",
      "Reading 1010.txt\n",
      "Reading 1011.txt\n",
      "Reading 1012.txt\n",
      "Reading 1013.txt\n",
      "Reading 1014.txt\n",
      "Reading 1015.txt\n",
      "Reading 1016.txt\n",
      "Reading 1017.txt\n",
      "Reading 1018.txt\n",
      "Reading 1019.txt\n",
      "Reading 1020.txt\n",
      "Reading 1021.txt\n",
      "Reading 1022.txt\n",
      "Reading 1023.txt\n",
      "Reading 1024.txt\n",
      "Reading 1025.txt\n",
      "Reading 1026.txt\n",
      "Reading 1027.txt\n",
      "Reading 1028.txt\n",
      "Reading 1029.txt\n",
      "Reading 1030.txt\n",
      "Reading 1031.txt\n",
      "Reading 1032.txt\n",
      "Reading 1033.txt\n",
      "Reading 1034.txt\n",
      "Reading 1035.txt\n",
      "Reading 1036.txt\n",
      "Reading 1037.txt\n",
      "Reading 1038.txt\n",
      "Reading 1039.txt\n",
      "Reading 1040.txt\n",
      "Reading 1041.txt\n",
      "Reading 1042.txt\n",
      "Reading 1043.txt\n",
      "Reading 1044.txt\n",
      "Reading 1045.txt\n",
      "Reading 1046.txt\n",
      "Reading 1047.txt\n",
      "Reading 1048.txt\n",
      "Reading 1049.txt\n",
      "Reading 1050.txt\n",
      "Reading 1051.txt\n",
      "Reading 1052.txt\n",
      "Reading 1053.txt\n",
      "Reading 1054.txt\n",
      "Reading 1055.txt\n",
      "Reading 1056.txt\n",
      "Reading 1057.txt\n",
      "Reading 1058.txt\n",
      "Reading 1059.txt\n",
      "Reading 1060.txt\n",
      "Reading 1061.txt\n",
      "Reading 1062.txt\n",
      "Reading 1063.txt\n",
      "Reading 1064.txt\n",
      "Reading 1065.txt\n",
      "Reading 1066.txt\n",
      "Reading 1067.txt\n",
      "Reading 1068.txt\n",
      "Reading 1069.txt\n",
      "Reading 1070.txt\n",
      "Reading 1071.txt\n",
      "Reading 1072.txt\n",
      "Reading 1073.txt\n",
      "Reading 1074.txt\n",
      "Reading 1075.txt\n",
      "Reading 1076.txt\n",
      "Reading 1077.txt\n",
      "Reading 1078.txt\n",
      "Reading 1079.txt\n",
      "Reading 1080.txt\n",
      "Reading 1081.txt\n",
      "Reading 1082.txt\n",
      "Reading 1083.txt\n",
      "Reading 1084.txt\n",
      "Reading 1085.txt\n",
      "Reading 1086.txt\n",
      "Reading 1087.txt\n",
      "Reading 1088.txt\n",
      "Reading 1089.txt\n",
      "Reading 1090.txt\n",
      "Reading 1091.txt\n",
      "Reading 1092.txt\n",
      "Reading 1093.txt\n",
      "Reading 1094.txt\n",
      "Reading 1095.txt\n",
      "Reading 1096.txt\n",
      "Reading 1097.txt\n",
      "Reading 1098.txt\n",
      "Reading 1099.txt\n",
      "Reading 1100.txt\n",
      "Reading 1101.txt\n",
      "Reading 1102.txt\n",
      "Reading 1103.txt\n",
      "Reading 1104.txt\n",
      "Reading 1105.txt\n",
      "Reading 1106.txt\n",
      "Reading 1107.txt\n",
      "Reading 1108.txt\n",
      "Reading 1109.txt\n",
      "Reading 1110.txt\n",
      "Reading 1111.txt\n",
      "Reading 1112.txt\n",
      "Reading 1113.txt\n",
      "Reading 1114.txt\n",
      "Reading 1115.txt\n",
      "Reading 1116.txt\n",
      "Reading 1117.txt\n",
      "Reading 1118.txt\n",
      "Reading 1119.txt\n",
      "Reading 1120.txt\n",
      "Reading 1121.txt\n",
      "Reading 1122.txt\n",
      "Reading 1123.txt\n",
      "Reading 1124.txt\n",
      "Reading 1125.txt\n",
      "Reading 1126.txt\n",
      "Reading 1127.txt\n",
      "Reading 1128.txt\n",
      "Reading 1136.txt\n",
      "Reading 1137.txt\n",
      "Reading 1138.txt\n",
      "Reading 1139.txt\n",
      "Reading 1140.txt\n",
      "Reading 1141.txt\n",
      "Reading 1142.txt\n",
      "Reading 1143.txt\n",
      "Reading 1144.txt\n",
      "Reading 1145.txt\n",
      "Reading 1146.txt\n",
      "Reading 1147.txt\n",
      "Reading 1148.txt\n",
      "Reading 1149.txt\n",
      "Reading 1150.txt\n",
      "Reading 1151.txt\n",
      "Reading 1152.txt\n",
      "Reading 1153.txt\n",
      "Reading 1154.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1155.txt\n",
      "Reading 1156.txt\n",
      "Reading 1157.txt\n",
      "Reading 1158.txt\n",
      "Reading 1159.txt\n",
      "Reading 1160.txt\n",
      "Reading 1161.txt\n",
      "Reading 1162.txt\n",
      "Reading 1163.txt\n",
      "Reading 1164.txt\n",
      "Reading 1165.txt\n",
      "Reading 1166.txt\n",
      "Reading 1167.txt\n",
      "Reading 1168.txt\n",
      "Reading 1169.txt\n",
      "Reading 1170.txt\n",
      "Reading 1171.txt\n",
      "Reading 1172.txt\n",
      "Reading 1173.txt\n",
      "Reading 1174.txt\n",
      "Reading 1175.txt\n",
      "Reading 1176.txt\n",
      "Reading 1177.txt\n",
      "Reading 1178.txt\n",
      "Reading 1179.txt\n",
      "Reading 1180.txt\n",
      "Reading 1181.txt\n",
      "Reading 1182.txt\n",
      "Reading 1183.txt\n",
      "Reading 1184.txt\n",
      "Reading 1185.txt\n",
      "Reading 1186.txt\n",
      "Reading 1187.txt\n",
      "Reading 1188.txt\n",
      "Reading 1189.txt\n",
      "Reading 1190.txt\n",
      "Reading 1191.txt\n",
      "Reading 1192.txt\n",
      "Reading 1193.txt\n",
      "Reading 1194.txt\n",
      "Reading 1196.txt\n",
      "Reading 1197.txt\n",
      "Reading 1198.txt\n",
      "Reading 1199.txt\n",
      "Reading 1200.txt\n",
      "Reading 1201.txt\n",
      "Reading 1202.txt\n",
      "Reading 1203.txt\n",
      "Reading 1204.txt\n",
      "Reading 1205.txt\n",
      "Reading 1206.txt\n",
      "Reading 1207.txt\n",
      "Reading 1208.txt\n",
      "Reading 1209.txt\n",
      "Reading 1210.txt\n",
      "Reading 1211.txt\n",
      "Reading 1212.txt\n",
      "Reading 1213.txt\n",
      "Reading 1214.txt\n",
      "Reading 1215.txt\n",
      "Reading 1216.txt\n",
      "Reading 1217.txt\n",
      "Reading 1218.txt\n",
      "Reading 1219.txt\n",
      "Reading 1220.txt\n",
      "Reading 1221.txt\n",
      "Reading 1222.txt\n",
      "Reading 1223.txt\n",
      "Reading 1224.txt\n",
      "Reading 1225.txt\n",
      "Reading 1226.txt\n",
      "Reading 1227.txt\n",
      "Reading 1228.txt\n",
      "Reading 1229.txt\n",
      "Reading 1230.txt\n",
      "Reading 1231.txt\n",
      "Reading 1232.txt\n",
      "Reading 1233.txt\n",
      "Reading 1234.txt\n",
      "Reading 1235.txt\n",
      "Reading 1236.txt\n",
      "Reading 1237.txt\n",
      "Reading 1238.txt\n",
      "Reading 1239.txt\n",
      "Reading 1240.txt\n",
      "Reading 1241.txt\n",
      "Reading 1242.txt\n",
      "Reading 1243.txt\n",
      "Reading 1244.txt\n",
      "Reading 1245.txt\n",
      "Reading 1246.txt\n",
      "Reading 1247.txt\n",
      "Reading 1248.txt\n",
      "Reading 1249.txt\n",
      "Reading 1250.txt\n",
      "Reading 1251.txt\n",
      "Reading 1252.txt\n",
      "Reading 1253.txt\n",
      "Reading 1254.txt\n",
      "Reading 1255.txt\n",
      "Reading 1256.txt\n",
      "Reading 1257.txt\n",
      "Reading 1259.txt\n",
      "Reading 1260.txt\n",
      "Reading 1261.txt\n",
      "Reading 1262.txt\n",
      "Reading 1263.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 nene kinakadena ng magulang sa Albay</td>\n",
       "      <td>Sa mga larawang ibinigay sa ABS-CBN Bicol, mak...</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hustisya hiling sa pagkamatay ng 'napagkamalan...</td>\n",
       "      <td>Nahuli sa CCTV ang pamamaril ng isang suspek s...</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MODUS: Mga batang babae pinaghuhubad kapalit n...</td>\n",
       "      <td>Noong isang beses, sinabihan siya ng anak na m...</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duterte dumepensa sa ulat ng 'lumalagong yaman'</td>\n",
       "      <td>Sa isang talumpati noong Sabado, sinabi ni Dut...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mga umuuwi para sa Semana Santa unti-unti nang...</td>\n",
       "      <td>Sa panayam sa DZMM, sinabi ni Armand Reyes, op...</td>\n",
       "      <td>Vehicles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0             2 nene kinakadena ng magulang sa Albay   \n",
       "0  Hustisya hiling sa pagkamatay ng 'napagkamalan...   \n",
       "0  MODUS: Mga batang babae pinaghuhubad kapalit n...   \n",
       "0    Duterte dumepensa sa ulat ng 'lumalagong yaman'   \n",
       "0  Mga umuuwi para sa Semana Santa unti-unti nang...   \n",
       "\n",
       "                                         Description  Category  \n",
       "0  Sa mga larawang ibinigay sa ABS-CBN Bicol, mak...     Crime  \n",
       "0  Nahuli sa CCTV ang pamamaril ng isang suspek s...     Crime  \n",
       "0  Noong isang beses, sinabihan siya ng anak na m...     Crime  \n",
       "0  Sa isang talumpati noong Sabado, sinabi ni Dut...  Politics  \n",
       "0  Sa panayam sa DZMM, sinabi ni Armand Reyes, op...  Vehicles  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dirName = 'Filipino Dataset';\n",
    "    \n",
    "# Get the list of all files in directory tree at given path\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "df_predata=pd.read_csv(\"filipino.csv\")\n",
    "df_predata=df_predata[[\"Headline\",\"Authors\",\"File Number\",\"Category\"]]\n",
    "df_predata=df_predata.rename(columns={'File Number':'file_number'}) \n",
    "\n",
    "df=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"])\n",
    "for row in df_predata.itertuples():\n",
    "    file_name=str(row.file_number)+\".txt\"\n",
    "    try:\n",
    "        file1 = open(dirName+\"/\"+file_name,\"r\",encoding=\"utf8\") \n",
    "    except:\n",
    "        continue\n",
    "    print(\"Reading \"+file_name)\n",
    "    sampleNews=file1.read().split(\"\\n\")\n",
    "    newsDesc=\"\"\n",
    "    for strline in sampleNews[1:len(sampleNews)]:\n",
    "        newsDesc+=strline\n",
    "    dfsample=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"],data=[[row.Headline,newsDesc,row.Category]])\n",
    "    df=df.append(dfsample)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the data into a pandas dataframe </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sa mga larawang ibinigay sa ABS-CBN Bicol, makikitang naglalaro ang magkapatid na 11 at 12 anyos pero nakakadena ang mga ito. Ayon sa nagbigay ng retrato, mag-iisang buwan na umano itong ginagawa sa mga bata. Nang puntahan ng Philippine National Police Women and Children\\'s Protection Desk (WCPD) ngayong Miyerkoles ang bahay ng mga biktima, umamin umano ang ina na kinakadena ang magkakapatid ng kanilang ama. Katuwiran ng ina na madalas umanong gumagala ang dalawang bata kaya kinakadena sila, pero nakakapaglaro naman sila sa labas ng bahay. Nasasangkot din umano sa pagnanakaw ang magkakapatid. Dagdag pa ng ina na hindi rin daw siya sigurado kung magbabago pa ang kaniyang mga anak.Pero iginiit ng ni Tabaco WCPD chief Police Capt. Maria Theresa Berdin, labag sa batas ang ginawa ng mga magulang. \"Very obvious naman na hindi tama na ikadena natin ang ating mga anak dahil lang sa rason na sutil or madalas umalis ng bahay dahil alam naman natin na ang kadena hindi akma sa tao,\" aniya. Hindi muna kinasuhan ang mga magulang pero regular daw silang bibisitahin ng PNP para hindi maulit ang pagkadena sa mga anak. Isasailalim din sa counseling ang pamilya.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index()\n",
    "df=df.drop(list(df)[0], axis=1)\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the Labels to Integers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Crime': 0, 'Politics': 1, 'Vehicles': 2, 'Business': 3, 'Entertainment': 4}\n",
      "Crime: 232\n",
      "Politics: 250\n",
      "Vehicles: 207\n",
      "Business: 248\n",
      "Entertainment: 250\n"
     ]
    }
   ],
   "source": [
    "df[\"Category\"]=df[\"Category\"].replace(\"Vehicle\", \"Vehicles\")\n",
    "df[\"Category\"]=df[\"Category\"].replace(\"Business \", \"Business\")\n",
    "categories=df[\"Category\"].unique().tolist()\n",
    "cat_dict=dict(zip(categories, range(0,len((categories)))))\n",
    "print(cat_dict)\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(df[df[\"Category\"]==x])))\n",
    "df[\"Category\"]=df['Category'].map(cat_dict, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing Special Characters </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sa mga larawang ibinigay sa ABS CBN Bicol  makikitang naglalaro ang magkapatid na    at    anyos pero nakakadena ang mga ito  Ayon sa nagbigay ng retrato  mag iisang buwan na umano itong ginagawa sa mga bata  Nang puntahan ng Philippine National Police Women and Children s Protection Desk  WCPD  ngayong Miyerkoles ang bahay ng mga biktima  umamin umano ang ina na kinakadena ang magkakapatid ng kanilang ama  Katuwiran ng ina na madalas umanong gumagala ang dalawang bata kaya kinakadena sila  pero nakakapaglaro naman sila sa labas ng bahay  Nasasangkot din umano sa pagnanakaw ang magkakapatid  Dagdag pa ng ina na hindi rin daw siya sigurado kung magbabago pa ang kaniyang mga anak Pero iginiit ng ni Tabaco WCPD chief Police Capt  Maria Theresa Berdin  labag sa batas ang ginawa ng mga magulang   Very obvious naman na hindi tama na ikadena natin ang ating mga anak dahil lang sa rason na sutil or madalas umalis ng bahay dahil alam naman natin na ang kadena hindi akma sa tao   aniya  Hindi muna kinasuhan ang mga magulang pero regular daw silang bibisitahin ng PNP para hindi maulit ang pagkadena sa mga anak  Isasailalim din sa counseling ang pamilya '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Description'] = df['Description'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df=df.dropna()\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 80-20 Train-Test Split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime: 184\n",
      "Politics: 202\n",
      "Vehicles: 162\n",
      "Business: 203\n",
      "Entertainment: 198\n",
      "Total Training Data: 949\n",
      "\n",
      "Crime: 48\n",
      "Politics: 48\n",
      "Vehicles: 45\n",
      "Business: 45\n",
      "Entertainment: 52\n",
      "Total Test Data: 238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df[\"Description\"], df[\"Category\"], test_size=0.20, random_state=42)\n",
    "\n",
    "train_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_train[y_train==cat_dict[x]])))\n",
    "    train_sum+=len(y_train[y_train==cat_dict[x]])\n",
    "print(\"Total Training Data: \"+str(train_sum)+\"\\n\")\n",
    "\n",
    "test_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_test[y_test==cat_dict[x]])))\n",
    "    test_sum+=len(y_test[y_test==cat_dict[x]])\n",
    "print(\"Total Test Data: \"+str(test_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ayon', 'kay', 'Police', 'Captain', 'Samuel', 'Bascon', 'hepe', 'ng', 'Tupi', 'Municipal', 'Police', 'Station', 'pasado', 'ala', 'ng', 'madaling', 'araw', 'nang', 'pumasok', 'si', 'Reylan', 'Solaiman', 'Katag', 'sa', 'isang', 'Community', 'Police', 'Assistance', 'Center', 'Compac', 'Sinira', 'umano', 'ng', 'suspek', 'ang', 'kawayang', 'bakod', 'sa', 'likod', 'ng', 'opisina', 'upang', 'makapasok', 'sabi', 'ni', 'Bascon', 'Sinita', 'ng', 'mga', 'naka', 'duty', 'na', 'pulis', 'si', 'Katag', 'pero', 'sa', 'halip', 'na', 'sabihin', 'ang', 'pakay', 'ay', 'nagpaputok', 'umano', 'ng', 'M', 'rifle', 'ang', 'suspek', 'kaya', 't', 'binaril', 'din', 'ng', 'mga', 'pulis', 'aniya', 'Napag', 'alaman', 'din', 'ng', 'mga', 'pulis', 'na', 'napatay', 'sa', 'isang', 'illegal', 'firearms', 'at', 'illegal', 'drug', 'raid', 'operations', 'ang', 'tiyuhin', 'nito', 'noong', 'nakaraang', 'taon', 'sa', 'Barangay', 'Polonuling', 'Hindi', 'naman', 'makapaniwala', 'ang', 'pamilya', 'ni', 'Katag', 'sa', 'mga', 'alegasyon', 'ng', 'pulis', 'laban', 'kanya', 'Nakahimlay', 'ngayon', 'ang', 'bangkay', 'ng', 'lalaki', 'sa', 'isa', 'sa', 'mga', 'punerarya', 'sa', 'bayan', 'Patuloy', 'pa', 'ang', 'ginagawang', 'imbestigasyon', 'ng', 'mga', 'pulisya', 'upang', 'malaman', 'ang', 'tunay', 'na', 'pakay', 'ng', 'lalaki']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_notcleaned=[word_tokenize(i) for i in X_train]\n",
    "y_train=y_train.values\n",
    "X_test_notcleaned=[word_tokenize(i) for i in X_test]\n",
    "y_test=y_test.values\n",
    "\n",
    "print(X_train_notcleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing of Stop Words and Words with length <3 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    s_cleaned=[]\n",
    "    f = open('tagalog_stop_words.csv','r')\n",
    "    d = f.read()\n",
    "    stop_words_filipino = d.split('\\n')\n",
    "    for w in s:\n",
    "        if w.lower() not in stopwords.words(\"english\") and len(w)>=3 and w.lower() not in stop_words_filipino:\n",
    "            s_cleaned.append(w)\n",
    " \n",
    "    return s_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ayon', 'kay', 'Police', 'Captain', 'Samuel', 'Bascon', 'hepe', 'Tupi', 'Municipal', 'Police', 'Station', 'pasado', 'ala', 'madaling', 'araw', 'pumasok', 'Reylan', 'Solaiman', 'Katag', 'isang', 'Community', 'Police', 'Assistance', 'Center', 'Compac', 'Sinira', 'umano', 'suspek', 'kawayang', 'bakod', 'likod', 'opisina', 'upang', 'makapasok', 'sabi', 'Bascon', 'Sinita', 'naka', 'duty', 'pulis', 'Katag', 'halip', 'sabihin', 'pakay', 'nagpaputok', 'umano', 'rifle', 'suspek', 'binaril', 'pulis', 'aniya', 'Napag', 'alaman', 'pulis', 'napatay', 'isang', 'illegal', 'firearms', 'illegal', 'drug', 'raid', 'operations', 'tiyuhin', 'nito', 'noong', 'nakaraang', 'taon', 'Barangay', 'Polonuling', 'makapaniwala', 'pamilya', 'Katag', 'alegasyon', 'pulis', 'laban', 'Nakahimlay', 'bangkay', 'lalaki', 'punerarya', 'bayan', 'Patuloy', 'ginagawang', 'imbestigasyon', 'pulisya', 'upang', 'malaman', 'tunay', 'pakay', 'lalaki']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import multiprocessing as mp\n",
    "import data_clean_filipino\n",
    "\n",
    "\n",
    "X_train,X_test=[],[]\n",
    "\n",
    "pool = mp.Pool(processes=4)\n",
    "X_train = pool.map(data_clean_filipino.remove_stop_words, X_train_notcleaned)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pool.map(data_clean.remove_stop_words, X_test_notcleaned)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4> <p> Creating the Vocabulary and word2index </p> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set([])\n",
    "for s in X_train:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Training and Test Data before fitting into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the GloVe word embeddings</h4>\n",
    "\n",
    "<p> You can download the word embeddings at https://nlp.stanford.edu/projects/glove/ </p>\n",
    "<br><b>Note:</b> Choose the 6B and 300D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_index=KeyedVectors.load_word2vec_format('fasttext_tagalog.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert all words to lowercase then to integers, then pad the sentences </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words:20796\n",
      "Max Length: 2012\n",
      "[2309 4405 1513 ...    0    0    0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X = [], []\n",
    "\n",
    "EMB_DIM=300\n",
    "num_words=len(word2index)+1\n",
    "print(\"Number of Words:\"+str(num_words))\n",
    "\n",
    "for s in X_train:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in X_test:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "#y_train=to_categorical(y_train)\n",
    "#y_test=to_categorical(y_test)\n",
    "print(train_sentences_X[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <h3>Creation of Pre-Trained Word Embeddings to be used for the embedding layer </h3> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((num_words,EMB_DIM))\n",
    "#print(word2index)\n",
    "for word,i in word2index.items():\n",
    "    if i>num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector=embeddings_index[word]\n",
    "    except:\n",
    "        continue\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTMs with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper of <i> Attention is All You Need </i>, we have the follow equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Attention(Q,K,V) &  = Softmax(\\frac{QK^T}{\\sqrt d_k}) V \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "TIME_STEPS=MAX_LENGTH\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = EMB_DIM\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard callback for visualizations of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./Graph', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Long-Short Term Memory without the Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2012, 300)         6238800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2012, 256)         439296    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2012, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 7,073,621\n",
      "Trainable params: 834,821\n",
      "Non-trainable params: 6,238,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, CuDNNLSTM,LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "model_lstm=Sequential() \n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "#embedding_layer=Embedding(num_words, 300,mask_zero=True)\n",
    "inputs=InputLayer(input_shape=(MAX_LENGTH, ))\n",
    "model_lstm.add(inputs)\n",
    "model_lstm.add(embedding_layer)\n",
    "model_lstm.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Bidirectional(LSTM(128)))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(len(np.unique(y_train)),activation=\"softmax\"))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model_lstm.summary())\n",
    "plot_model(model_lstm, to_file='model_lstm_filipino.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 949 samples, validate on 238 samples\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0.003)\n",
    "history_lstm=model_lstm.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=32,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph_LSTM_Filipino')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 71s 160ms/step\n",
      "['loss', 'acc']\n",
      "[0.15417643115761576, 0.9505617992261822]\n"
     ]
    }
   ],
   "source": [
    "#model_lstm.save(\"LSTM.h5\")\n",
    "scores = model_lstm.evaluate(test_sentences_X,y_test)\n",
    "print(model_lstm.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 81s 183ms/step\n",
      "['loss', 'acc']\n",
      "[0.15417643115761576, 0.9505617992261822]\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "model_lstm.save_weights('lstm_weights_Filipino.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('lstm_architecture_Filipino.json', 'w') as f:\n",
    "    f.write(model_lstm.to_json())\n",
    "    \n",
    "from keras.models import model_from_json\n",
    "del model_lstm\n",
    "# Model reconstruction from JSON file\n",
    "with open('lstm_architecture_Filipino.json', 'r') as f:\n",
    "    model_lstm = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model_lstm.load_weights('lstm_weights_Filipino.h5')\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "scores = model_lstm.evaluate(test_sentences_X,y_test)\n",
    "print(model_lstm.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history_lstm.history['acc'])\n",
    "plt.plot(history_lstm.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_lstm_fil.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_lstm_fil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Long-Short  Term Memory with Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1749)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1749, 300)    7512900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1749, 256)    439296      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1749, 256)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1749, 256)    394240      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 256, 1749)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256, 1749)    3060750     permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 1749, 256)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1749, 256)    0           bidirectional_2[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1749, 256)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 447744)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            2238725     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,645,911\n",
      "Trainable params: 6,133,011\n",
      "Non-trainable params: 7,512,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge,concatenate,add,dot,multiply, Dense,CuDNNLSTM, LSTM, InputLayer, Bidirectional, Embedding, Activation,Dropout\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import plot_model\n",
    "\n",
    "inputs = Input(shape=(MAX_LENGTH,))\n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "attention_mul = embedding_layer(inputs)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul = attention_3d_block(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(len(np.unique(y_train)),activation=\"softmax\")(attention_mul)\n",
    "model = Model(inputs=[inputs], outputs=output)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_attention_Filipino.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/15\n",
      "1780/1780 [==============================] - 1782s 1s/step - loss: 1.6072 - acc: 0.5067 - val_loss: 1.5977 - val_acc: 0.5438\n",
      "Epoch 2/15\n",
      "1780/1780 [==============================] - 1769s 994ms/step - loss: 1.4248 - acc: 0.4803 - val_loss: 1.0760 - val_acc: 0.5910\n",
      "Epoch 3/15\n",
      "1780/1780 [==============================] - 1771s 995ms/step - loss: 0.9439 - acc: 0.6073 - val_loss: 0.8150 - val_acc: 0.6854\n",
      "Epoch 4/15\n",
      "1780/1780 [==============================] - 1767s 993ms/step - loss: 0.7496 - acc: 0.7090 - val_loss: 0.6944 - val_acc: 0.7146\n",
      "Epoch 5/15\n",
      "1780/1780 [==============================] - 1757s 987ms/step - loss: 0.6049 - acc: 0.7534 - val_loss: 0.5415 - val_acc: 0.7753\n",
      "Epoch 6/15\n",
      "1780/1780 [==============================] - 1747s 981ms/step - loss: 0.5039 - acc: 0.8084 - val_loss: 0.4407 - val_acc: 0.8517\n",
      "Epoch 7/15\n",
      "1780/1780 [==============================] - 1765s 992ms/step - loss: 0.3873 - acc: 0.9034 - val_loss: 0.3863 - val_acc: 0.9056\n",
      "Epoch 8/15\n",
      "1780/1780 [==============================] - 1744s 980ms/step - loss: 0.3369 - acc: 0.9219 - val_loss: 0.3272 - val_acc: 0.9348\n",
      "Epoch 9/15\n",
      "1780/1780 [==============================] - 1774s 997ms/step - loss: 0.2777 - acc: 0.9461 - val_loss: 0.2709 - val_acc: 0.9326\n",
      "Epoch 10/15\n",
      "1780/1780 [==============================] - 1765s 991ms/step - loss: 0.2148 - acc: 0.9534 - val_loss: 0.2371 - val_acc: 0.9416\n",
      "Epoch 11/15\n",
      "1780/1780 [==============================] - 1747s 981ms/step - loss: 0.3552 - acc: 0.9090 - val_loss: 0.2297 - val_acc: 0.9258\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1)\n",
    "history=model.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=32,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph_Filipino')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 93s 208ms/step\n",
      "['loss', 'acc']\n",
      "[0.22971181260066087, 0.9258426968970995]\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "model.save_weights('attention_weights_Filipino.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('attention_architecture_Filipino.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "from keras.models import model_from_json\n",
    "del model\n",
    "# Model reconstruction from JSON file\n",
    "with open('attention_architecture_Filipino.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('attention_weights_Filipino.h5')\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "scores = model.evaluate(test_sentences_X,y_test)\n",
    "print(model.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_attention_fil.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_attention_fil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'is', 'in', 'intensive', 'care', 'after', 'suffering', 'a', 'stroke', '.', 'known', 'for', 'movies', 'including', 'Boyz', 'N', 'The', 'Hood', 'and', '2', 'Fast', '2', 'Furious', ',', 'became', 'unwell', 'on', 'Wednesday', ',', 'his', 'family', 'said', '.', 'Following', 'news', 'of', 'the', 'stroke', ',', 'friends', 'and', 'colleagues', 'sent', 'their', 'best', 'wishes', '.', 'Rapper', 'Snoop', 'Dogg', 'shared', 'a', 'picture', 'of', 'the', 'pair', 'together', 'on', 'Instagram', ',', 'writing', ':', 'Pray', '4', 'my', 'brother', '.']\n",
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'intensive', 'care', 'suffering', 'stroke', 'known', 'movies', 'including', 'Boyz', 'Hood', 'Fast', 'Furious', 'became', 'unwell', 'Wednesday', 'family', 'said', 'Following', 'news', 'stroke', 'friends', 'colleagues', 'sent', 'best', 'wishes', 'Rapper', 'Snoop', 'Dogg', 'shared', 'picture', 'pair', 'together', 'Instagram', 'writing', 'Pray', 'brother']\n",
      "[1, 18319, 17823, 1, 3156, 12852, 17711, 18704, 3781, 4997, 16918, 1, 4809, 5163, 21423, 13750, 3787, 7131, 2788, 18581, 10005, 15050, 18704, 13666, 13235, 10162, 22430, 20638, 6068, 13899, 10982, 12850, 16746, 2396, 21272, 1, 636, 918, 17689]\n",
      "[[    1 18319 17823 ...     0     0     0]]\n",
      "[[0.00194192 0.8597092  0.01367367 0.09096657 0.03370867]]\n"
     ]
    }
   ],
   "source": [
    "from attention_utils import get_activations, get_data_recurrent\n",
    "attention_vectors = []\n",
    "\n",
    "input_text=\"Oscar-nominated director John Singleton is in intensive care after suffering a stroke. known for movies including Boyz N The Hood and 2 Fast 2 Furious, became unwell on Wednesday, his family said. Following news of the stroke, friends and colleagues sent their best wishes. Rapper Snoop Dogg shared a picture of the pair together on Instagram, writing: Pray 4 my brother. \" \n",
    "input_text=input_text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "input_text=word_tokenize(input_text)\n",
    "print(input_text)\n",
    "input_text_cleaned=[]\n",
    "for w in input_text:\n",
    "    if w.lower() not in stopwords.words(\"english\") and len(w)>=3:\n",
    "        input_text_cleaned.append(w)\n",
    "print(input_text_cleaned)\n",
    "input_text_index=[]\n",
    "for w in input_text_cleaned:\n",
    "    try:\n",
    "        input_text_index.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        input_text_index.append(word2index['-OOV-'])\n",
    "print(input_text_index)\n",
    "input_text_pad=pad_sequences([input_text_index], maxlen=MAX_LENGTH, padding='post')\n",
    "print(input_text_pad)\n",
    "\n",
    "print(model.predict(input_text_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 50, 30, 187, 55, 22, 21, 33, 11, 35]\n",
      "{'colleagues': 0.00027969663, 'Dogg': 0.00029982178, 'stroke': 0.0006032483, 'news': 0.0033030058, 'pair': 0.1252957, 'Boyz': 0.3683312, 'Instagram': 0.43804774}\n"
     ]
    }
   ],
   "source": [
    "attention_vector = np.mean(get_activations(model,input_text_pad,print_shape_only=True,layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "\n",
    "words_index=sorted(range(len(attention_vector)), key=lambda i: attention_vector[i])[-10:]\n",
    "print(words_index)\n",
    "top_words=dict()\n",
    "for x in words_index:\n",
    "    if x<len(input_text_cleaned):\n",
    "        top_words[input_text_cleaned[x]]=attention_vector[x]\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
