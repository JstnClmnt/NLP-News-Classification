{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing, Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = dirName+\"/\"+entry\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1.txt\n",
      "Reading 2.txt\n",
      "Reading 3.txt\n",
      "Reading 4.txt\n",
      "Reading 5.txt\n",
      "Reading 7.txt\n",
      "Reading 8.txt\n",
      "Reading 9.txt\n",
      "Reading 10.txt\n",
      "Reading 11.txt\n",
      "Reading 12.txt\n",
      "Reading 15.txt\n",
      "Reading 16.txt\n",
      "Reading 19.txt\n",
      "Reading 20.txt\n",
      "Reading 21.txt\n",
      "Reading 22.txt\n",
      "Reading 23.txt\n",
      "Reading 24.txt\n",
      "Reading 25.txt\n",
      "Reading 26.txt\n",
      "Reading 27.txt\n",
      "Reading 28.txt\n",
      "Reading 29.txt\n",
      "Reading 30.txt\n",
      "Reading 31.txt\n",
      "Reading 32.txt\n",
      "Reading 33.txt\n",
      "Reading 34.txt\n",
      "Reading 35.txt\n",
      "Reading 36.txt\n",
      "Reading 37.txt\n",
      "Reading 38.txt\n",
      "Reading 39.txt\n",
      "Reading 40.txt\n",
      "Reading 41.txt\n",
      "Reading 42.txt\n",
      "Reading 43.txt\n",
      "Reading 44.txt\n",
      "Reading 45.txt\n",
      "Reading 46.txt\n",
      "Reading 47.txt\n",
      "Reading 48.txt\n",
      "Reading 49.txt\n",
      "Reading 50.txt\n",
      "Reading 51.txt\n",
      "Reading 52.txt\n",
      "Reading 53.txt\n",
      "Reading 54.txt\n",
      "Reading 55.txt\n",
      "Reading 56.txt\n",
      "Reading 57.txt\n",
      "Reading 58.txt\n",
      "Reading 59.txt\n",
      "Reading 60.txt\n",
      "Reading 61.txt\n",
      "Reading 62.txt\n",
      "Reading 63.txt\n",
      "Reading 64.txt\n",
      "Reading 65.txt\n",
      "Reading 66.txt\n",
      "Reading 67.txt\n",
      "Reading 68.txt\n",
      "Reading 69.txt\n",
      "Reading 70.txt\n",
      "Reading 71.txt\n",
      "Reading 72.txt\n",
      "Reading 73.txt\n",
      "Reading 74.txt\n",
      "Reading 75.txt\n",
      "Reading 76.txt\n",
      "Reading 77.txt\n",
      "Reading 78.txt\n",
      "Reading 79.txt\n",
      "Reading 80.txt\n",
      "Reading 81.txt\n",
      "Reading 82.txt\n",
      "Reading 83.txt\n",
      "Reading 84.txt\n",
      "Reading 85.txt\n",
      "Reading 86.txt\n",
      "Reading 87.txt\n",
      "Reading 88.txt\n",
      "Reading 89.txt\n",
      "Reading 90.txt\n",
      "Reading 91.txt\n",
      "Reading 92.txt\n",
      "Reading 93.txt\n",
      "Reading 94.txt\n",
      "Reading 95.txt\n",
      "Reading 96.txt\n",
      "Reading 97.txt\n",
      "Reading 98.txt\n",
      "Reading 99.txt\n",
      "Reading 100.txt\n",
      "Reading 101.txt\n",
      "Reading 102.txt\n",
      "Reading 103.txt\n",
      "Reading 104.txt\n",
      "Reading 105.txt\n",
      "Reading 106.txt\n",
      "Reading 107.txt\n",
      "Reading 108.txt\n",
      "Reading 109.txt\n",
      "Reading 110.txt\n",
      "Reading 111.txt\n",
      "Reading 112.txt\n",
      "Reading 113.txt\n",
      "Reading 114.txt\n",
      "Reading 115.txt\n",
      "Reading 116.txt\n",
      "Reading 117.txt\n",
      "Reading 118.txt\n",
      "Reading 119.txt\n",
      "Reading 120.txt\n",
      "Reading 121.txt\n",
      "Reading 122.txt\n",
      "Reading 123.txt\n",
      "Reading 124.txt\n",
      "Reading 125.txt\n",
      "Reading 126.txt\n",
      "Reading 127.txt\n",
      "Reading 128.txt\n",
      "Reading 129.txt\n",
      "Reading 130.txt\n",
      "Reading 131.txt\n",
      "Reading 132.txt\n",
      "Reading 133.txt\n",
      "Reading 134.txt\n",
      "Reading 135.txt\n",
      "Reading 136.txt\n",
      "Reading 137.txt\n",
      "Reading 138.txt\n",
      "Reading 139.txt\n",
      "Reading 140.txt\n",
      "Reading 141.txt\n",
      "Reading 142.txt\n",
      "Reading 143.txt\n",
      "Reading 144.txt\n",
      "Reading 145.txt\n",
      "Reading 146.txt\n",
      "Reading 147.txt\n",
      "Reading 148.txt\n",
      "Reading 149.txt\n",
      "Reading 150.txt\n",
      "Reading 151.txt\n",
      "Reading 152.txt\n",
      "Reading 153.txt\n",
      "Reading 154.txt\n",
      "Reading 155.txt\n",
      "Reading 156.txt\n",
      "Reading 157.txt\n",
      "Reading 158.txt\n",
      "Reading 159.txt\n",
      "Reading 160.txt\n",
      "Reading 161.txt\n",
      "Reading 162.txt\n",
      "Reading 163.txt\n",
      "Reading 164.txt\n",
      "Reading 165.txt\n",
      "Reading 166.txt\n",
      "Reading 167.txt\n",
      "Reading 168.txt\n",
      "Reading 169.txt\n",
      "Reading 170.txt\n",
      "Reading 171.txt\n",
      "Reading 172.txt\n",
      "Reading 173.txt\n",
      "Reading 174.txt\n",
      "Reading 175.txt\n",
      "Reading 176.txt\n",
      "Reading 177.txt\n",
      "Reading 178.txt\n",
      "Reading 179.txt\n",
      "Reading 180.txt\n",
      "Reading 181.txt\n",
      "Reading 182.txt\n",
      "Reading 183.txt\n",
      "Reading 184.txt\n",
      "Reading 185.txt\n",
      "Reading 186.txt\n",
      "Reading 187.txt\n",
      "Reading 188.txt\n",
      "Reading 189.txt\n",
      "Reading 190.txt\n",
      "Reading 191.txt\n",
      "Reading 192.txt\n",
      "Reading 193.txt\n",
      "Reading 194.txt\n",
      "Reading 195.txt\n",
      "Reading 196.txt\n",
      "Reading 197.txt\n",
      "Reading 198.txt\n",
      "Reading 199.txt\n",
      "Reading 200.txt\n",
      "Reading 201.txt\n",
      "Reading 202.txt\n",
      "Reading 203.txt\n",
      "Reading 204.txt\n",
      "Reading 205.txt\n",
      "Reading 206.txt\n",
      "Reading 207.txt\n",
      "Reading 208.txt\n",
      "Reading 209.txt\n",
      "Reading 210.txt\n",
      "Reading 211.txt\n",
      "Reading 212.txt\n",
      "Reading 213.txt\n",
      "Reading 214.txt\n",
      "Reading 215.txt\n",
      "Reading 216.txt\n",
      "Reading 217.txt\n",
      "Reading 218.txt\n",
      "Reading 219.txt\n",
      "Reading 220.txt\n",
      "Reading 221.txt\n",
      "Reading 222.txt\n",
      "Reading 223.txt\n",
      "Reading 224.txt\n",
      "Reading 225.txt\n",
      "Reading 226.txt\n",
      "Reading 227.txt\n",
      "Reading 228.txt\n",
      "Reading 229.txt\n",
      "Reading 230.txt\n",
      "Reading 231.txt\n",
      "Reading 232.txt\n",
      "Reading 233.txt\n",
      "Reading 234.txt\n",
      "Reading 235.txt\n",
      "Reading 236.txt\n",
      "Reading 237.txt\n",
      "Reading 238.txt\n",
      "Reading 239.txt\n",
      "Reading 240.txt\n",
      "Reading 241.txt\n",
      "Reading 242.txt\n",
      "Reading 243.txt\n",
      "Reading 244.txt\n",
      "Reading 245.txt\n",
      "Reading 246.txt\n",
      "Reading 247.txt\n",
      "Reading 248.txt\n",
      "Reading 249.txt\n",
      "Reading 251.txt\n",
      "Reading 252.txt\n",
      "Reading 253.txt\n",
      "Reading 254.txt\n",
      "Reading 255.txt\n",
      "Reading 256.txt\n",
      "Reading 257.txt\n",
      "Reading 258.txt\n",
      "Reading 259.txt\n",
      "Reading 260.txt\n",
      "Reading 261.txt\n",
      "Reading 262.txt\n",
      "Reading 263.txt\n",
      "Reading 264.txt\n",
      "Reading 265.txt\n",
      "Reading 266.txt\n",
      "Reading 267.txt\n",
      "Reading 268.txt\n",
      "Reading 269.txt\n",
      "Reading 270.txt\n",
      "Reading 271.txt\n",
      "Reading 272.txt\n",
      "Reading 273.txt\n",
      "Reading 274.txt\n",
      "Reading 275.txt\n",
      "Reading 276.txt\n",
      "Reading 277.txt\n",
      "Reading 278.txt\n",
      "Reading 279.txt\n",
      "Reading 280.txt\n",
      "Reading 281.txt\n",
      "Reading 282.txt\n",
      "Reading 283.txt\n",
      "Reading 284.txt\n",
      "Reading 285.txt\n",
      "Reading 286.txt\n",
      "Reading 287.txt\n",
      "Reading 288.txt\n",
      "Reading 289.txt\n",
      "Reading 290.txt\n",
      "Reading 291.txt\n",
      "Reading 292.txt\n",
      "Reading 293.txt\n",
      "Reading 294.txt\n",
      "Reading 295.txt\n",
      "Reading 296.txt\n",
      "Reading 297.txt\n",
      "Reading 298.txt\n",
      "Reading 299.txt\n",
      "Reading 300.txt\n",
      "Reading 301.txt\n",
      "Reading 302.txt\n",
      "Reading 303.txt\n",
      "Reading 304.txt\n",
      "Reading 307.txt\n",
      "Reading 308.txt\n",
      "Reading 309.txt\n",
      "Reading 310.txt\n",
      "Reading 318.txt\n",
      "Reading 319.txt\n",
      "Reading 326.txt\n",
      "Reading 328.txt\n",
      "Reading 330.txt\n",
      "Reading 332.txt\n",
      "Reading 333.txt\n",
      "Reading 334.txt\n",
      "Reading 335.txt\n",
      "Reading 336.txt\n",
      "Reading 337.txt\n",
      "Reading 338.txt\n",
      "Reading 339.txt\n",
      "Reading 340.txt\n",
      "Reading 341.txt\n",
      "Reading 343.txt\n",
      "Reading 344.txt\n",
      "Reading 345.txt\n",
      "Reading 346.txt\n",
      "Reading 347.txt\n",
      "Reading 348.txt\n",
      "Reading 350.txt\n",
      "Reading 351.txt\n",
      "Reading 352.txt\n",
      "Reading 353.txt\n",
      "Reading 354.txt\n",
      "Reading 355.txt\n",
      "Reading 356.txt\n",
      "Reading 357.txt\n",
      "Reading 358.txt\n",
      "Reading 359.txt\n",
      "Reading 360.txt\n",
      "Reading 361.txt\n",
      "Reading 362.txt\n",
      "Reading 363.txt\n",
      "Reading 364.txt\n",
      "Reading 365.txt\n",
      "Reading 366.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf1 in position 442: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-151ae1744a65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfile1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirName\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reading \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msampleNews\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mnewsDesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstrline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampleNews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampleNews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf1 in position 442: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dirName = 'Filipino Dataset';\n",
    "    \n",
    "# Get the list of all files in directory tree at given path\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "df_predata=pd.read_csv(\"filipino.csv\")\n",
    "df_predata=df_predata[[\"Headline\",\"Authors\",\"File Number\",\"Category\"]]\n",
    "df_predata=df_predata.rename(columns={'File Number':'file_number'}) \n",
    "\n",
    "df=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"])\n",
    "for row in df_predata.itertuples():\n",
    "    file_name=str(row.file_number)+\".txt\"\n",
    "    file1 = open(dirName+\"/\"+file_name,\"r\",encoding=\"utf8\") \n",
    "    print(\"Reading \"+file_name)\n",
    "    sampleNews=file1.read().split(\"\\n\")\n",
    "    newsDesc=\"\"\n",
    "    for strline in sampleNews[1:len(sampleNews)]:\n",
    "        newsDesc+=strline\n",
    "    dfsample=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"],data=[[row.Headline,newsDesc,row.Category]])\n",
    "    df=df.append(dfsample)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the data into a pandas dataframe </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dirName = 'Filipino Dataset';\n",
    "    \n",
    "# Get the list of all files in directory tree at given path\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "    \n",
    "df_predata=pd.read_csv(\"filipino.csv\")\n",
    "df=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"])\n",
    "# Print the files    \n",
    "\n",
    "for elem in listOfFiles:\n",
    "    file1 = open(elem,\"r\") \n",
    "    sampleNews=file1.read().split(\"\\n\")\n",
    "    newsDesc=\"\"\n",
    "    for strline in sampleNews[1:len(sampleNews)]:\n",
    "        newsDesc+=strline\n",
    "    \"\"\"\n",
    "    print(\"Category: \"+elem.split(\"/\")[1])\n",
    "    print(\"Title:\"+sampleNews[0])\n",
    "    print(\"Description:\"+newsDesc[0:50])\n",
    "    \"\"\"\n",
    "    dfsample=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"],data=[[sampleNews[0],newsDesc,elem.split(\"/\")[1]]])\n",
    "    df=df.append(dfsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL\\'s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL\\'s existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann\\'s purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index()\n",
    "df=df.drop(list(df)[0], axis=1)\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Domec...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0  Ad sales boost Time Warner profit   \n",
       "1   Dollar gains on Greenspan speech   \n",
       "2  Yukos unit buyer faces loan claim   \n",
       "3  High fuel prices hit BA's profits   \n",
       "4  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                         Description  Category  \n",
       "0  Quarterly profits at US media giant TimeWarner...  business  \n",
       "1  The dollar has hit its highest level against t...  business  \n",
       "2  The owners of embattled Russian oil giant Yuko...  business  \n",
       "3  British Airways has blamed high fuel prices fo...  business  \n",
       "4  Shares in UK drinks and food firm Allied Domec...  business  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the Labels to Integers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n",
      "business: 510\n",
      "entertainment: 386\n",
      "politics: 417\n",
      "sport: 511\n",
      "tech: 401\n"
     ]
    }
   ],
   "source": [
    "categories=df[\"Category\"].unique().tolist()\n",
    "cat_dict=dict(zip(categories, range(0,len((categories)))))\n",
    "print(cat_dict)\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(df[df[\"Category\"]==x])))\n",
    "df[\"Category\"]=df['Category'].map(cat_dict, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing Special Characters </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quarterly profits at US media giant TimeWarner jumped     to      bn       m  for the three months to December  from     m year earlier The firm  which is now one of the biggest investors in Google  benefited from sales of high speed internet connections and higher advert sales  TimeWarner said fourth quarter sales rose    to      bn from      bn  Its profits were buoyed by one off gains which offset a profit dip at Warner Bros  and less users for AOL Time Warner said on Friday that it now owns    of search engine Google  But its own internet business  AOL  had has mixed fortunes  It lost         subscribers in the fourth quarter profits were lower than in the preceding three quarters  However  the company said AOL s underlying profit before exceptional items rose    on the back of stronger internet advertising revenues  It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL s existing customers for high speed broadband  TimeWarner also has to restate      and      results following a probe by the US Securities Exchange Commission  SEC   which is close to concluding Time Warner s fourth quarter profits were slightly better than analysts  expectations  But its film division saw profits slump     to     m  helped by box office flops Alexander and Catwoman  a sharp contrast to year earlier  when the third and final film in the Lord of the Rings trilogy boosted results  For the full year  TimeWarner posted a profit of      bn  up     from its      performance  while revenues grew      to       bn   Our financial performance was strong  meeting or exceeding all of our full year objectives and greatly enhancing our flexibility   chairman and chief executive Richard Parsons said  For       TimeWarner is projecting operating earnings growth of around     and also expects higher revenue and wider profit margins TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators  It has already offered to pay     m to settle charges  in a deal that is under review by the SEC  The company said it was unable to estimate the amount it needed to set aside for legal reserves  which it previously set at     m  It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann s purchase of a stake in AOL Europe  which it had reported as advertising revenue  It will now book the sale of its stake in AOL Europe as a loss on the value of that stake '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Description'] = df['Description'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df=df.dropna()\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 80-20 Train-Test Split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: 395\n",
      "entertainment: 314\n",
      "politics: 341\n",
      "sport: 409\n",
      "tech: 321\n",
      "Total Training Data: 1780\n",
      "\n",
      "business: 115\n",
      "entertainment: 72\n",
      "politics: 76\n",
      "sport: 102\n",
      "tech: 80\n",
      "Total Test Data: 445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df[\"Description\"], df[\"Category\"], test_size=0.20, random_state=42)\n",
    "\n",
    "train_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_train[y_train==cat_dict[x]])))\n",
    "    train_sum+=len(y_train[y_train==cat_dict[x]])\n",
    "print(\"Total Training Data: \"+str(train_sum)+\"\\n\")\n",
    "\n",
    "test_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_test[y_test==cat_dict[x]])))\n",
    "    test_sum+=len(y_test[y_test==cat_dict[x]])\n",
    "print(\"Total Test Data: \"+str(test_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ashley', 'Cole', 'has', 'refused', 'to', 'blame', 'Robin', 'van', 'Persie', 'for', 'leaving', 'Arsenal', 'with', 'no', 'fully', 'fit', 'strikers', 'for', 'the', 'FA', 'Cup', 'fifth', 'round', 'replay', 'at', 'Sheffield', 'United', 'Van', 'Persie', 'is', 'suspended', 'alongside', 'Dennis', 'Bergkamp', 'and', 'Jose', 'Antonio', 'Reyes', 'after', 'being', 'sent', 'off', 'at', 'Southampton', 'when', 'Arsenal', 'had', 'a', 'numerical', 'advantage', 'Thierry', 'Henry', 'is', 'ruled', 'out', 'with', 'an', 'Achilles', 'tendon', 'injury', 'but', 'Cole', 'said', 'No', 'one', 'is', 'putting', 'the', 'blame', 'on', 'Robin', 'It', 's', 'just', 'something', 'that', 'happens', 'on', 'the', 'spur', 'of', 'the', 'moment', 'Cole', 'added', 'I', 've', 'done', 'it', 'before', 'and', 'I', 'hope', 'they', 'didn', 't', 'blame', 'me', 'for', 'anything', 'Of', 'course', 'he', 'll', 'learn', 'I', 've', 'been', 'sent', 'off', 'a', 'couple', 'of', 'times', 'now', 'and', 'it', 's', 'just', 'one', 'of', 'those', 'things', 'when', 'you', 'go', 'a', 'bit', 'crazy', 'for', 'one', 'or', 'two', 'seconds', 'Freddie', 'Ljungberg', 'is', 'likely', 'to', 'be', 'used', 'in', 'an', 'emergency', 'striking', 'role', 'and', 'will', 'be', 'partnered', 'by', 'either', 'Arturo', 'Lupoli', 'Quincy', 'Owusu', 'Abeyie', 'or', 'Jeremie', 'Aliadiere', 'Gunners', 'boss', 'Arsene', 'Wenger', 'said', 'Freddie', 'is', 'an', 'option', 'but', 'we', 'need', 'a', 'second', 'striker', 'I', 'have', 'to', 'decide', 'whether', 'it', 'will', 'be', 'Aliadiere', 'Quincy', 'or', 'Lupoli', 'who', 'will', 'start', 'with', 'him', 'up', 'front', 'Those', 'three', 'will', 'be', 'involved', 'Arsenal', 'are', 'also', 'without', 'winger', 'Robert', 'Pires', 'who', 'sustained', 'an', 'ankle', 'injury', 'at', 'St', 'Mary', 's', 'Wenger', 'added', 'It', 'doesn', 't', 'look', 'like', 'anything', 'is', 'fractured', 'but', 'it', 'is', 'a', 'good', 'ankle', 'sprain', 'It', 'does', 'not', 'look', 'like', 'Pires', 'will', 'be', 'ready', 'for', 'two', 'to', 'three', 'weeks']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_notcleaned=[word_tokenize(i) for i in X_train]\n",
    "y_train=y_train.values\n",
    "X_test_notcleaned=[word_tokenize(i) for i in X_test]\n",
    "y_test=y_test.values\n",
    "\n",
    "print(X_train_notcleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing of Stop Words and Words with length <3 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    s_cleaned=[]\n",
    "    for w in s:\n",
    "        if w.lower() not in stopwords.words(\"english\") and len(w)>=3:\n",
    "            s_cleaned.append(w)\n",
    " \n",
    "    return s_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ashley', 'Cole', 'refused', 'blame', 'Robin', 'van', 'Persie', 'leaving', 'Arsenal', 'fully', 'fit', 'strikers', 'Cup', 'fifth', 'round', 'replay', 'Sheffield', 'United', 'Van', 'Persie', 'suspended', 'alongside', 'Dennis', 'Bergkamp', 'Jose', 'Antonio', 'Reyes', 'sent', 'Southampton', 'Arsenal', 'numerical', 'advantage', 'Thierry', 'Henry', 'ruled', 'Achilles', 'tendon', 'injury', 'Cole', 'said', 'one', 'putting', 'blame', 'Robin', 'something', 'happens', 'spur', 'moment', 'Cole', 'added', 'done', 'hope', 'blame', 'anything', 'course', 'learn', 'sent', 'couple', 'times', 'one', 'things', 'bit', 'crazy', 'one', 'two', 'seconds', 'Freddie', 'Ljungberg', 'likely', 'used', 'emergency', 'striking', 'role', 'partnered', 'either', 'Arturo', 'Lupoli', 'Quincy', 'Owusu', 'Abeyie', 'Jeremie', 'Aliadiere', 'Gunners', 'boss', 'Arsene', 'Wenger', 'said', 'Freddie', 'option', 'need', 'second', 'striker', 'decide', 'whether', 'Aliadiere', 'Quincy', 'Lupoli', 'start', 'front', 'three', 'involved', 'Arsenal', 'also', 'without', 'winger', 'Robert', 'Pires', 'sustained', 'ankle', 'injury', 'Mary', 'Wenger', 'added', 'look', 'like', 'anything', 'fractured', 'good', 'ankle', 'sprain', 'look', 'like', 'Pires', 'ready', 'two', 'three', 'weeks']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import multiprocessing as mp\n",
    "import data_clean\n",
    "\n",
    "\n",
    "X_train,X_test=[],[]\n",
    "\n",
    "pool = mp.Pool(processes=4)\n",
    "X_train = pool.map(data_clean.remove_stop_words, X_train_notcleaned)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pool.map(data_clean.remove_stop_words, X_test_notcleaned)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4> <p> Creating the Vocabulary and word2index </p> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set([])\n",
    "for s in X_train:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Training and Test Data before fitting into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the GloVe word embeddings</h4>\n",
    "\n",
    "<p> You can download the word embeddings at https://nlp.stanford.edu/projects/glove/ </p>\n",
    "<br><b>Note:</b> Choose the 6B and 300D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt',encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert all words to lowercase then to integers, then pad the sentences </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words:25043\n",
      "Max Length: 1749\n",
      "[ 5387 13272 15238 ...     0     0     0]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X = [], []\n",
    "\n",
    "EMB_DIM=300\n",
    "num_words=len(word2index)+1\n",
    "print(\"Number of Words:\"+str(num_words))\n",
    "\n",
    "for s in X_train:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in X_test:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "#y_train=to_categorical(y_train)\n",
    "#y_test=to_categorical(y_test)\n",
    "print(train_sentences_X[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <h3>Creation of Pre-Trained Word Embeddings to be used for the embedding layer </h3> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((num_words,EMB_DIM))\n",
    "#print(word2index)\n",
    "for word,i in word2index.items():\n",
    "    if i>num_words:\n",
    "        continue\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTMs with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper of <i> Attention is All You Need </i>, we have the follow equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Attention(Q,K,V) &  = Softmax(\\frac{QK^T}{\\sqrt d_k}) V \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "TIME_STEPS=MAX_LENGTH\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = EMB_DIM\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard callback for visualizations of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./Graph', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Long-Short Term Memory without the Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1749, 300)         7512900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1749, 256)         439296    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1749, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 8,347,721\n",
      "Trainable params: 834,821\n",
      "Non-trainable params: 7,512,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, CuDNNLSTM,LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "model_lstm=Sequential() \n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "#embedding_layer=Embedding(num_words, 300,mask_zero=True)\n",
    "inputs=InputLayer(input_shape=(MAX_LENGTH, ))\n",
    "model_lstm.add(inputs)\n",
    "model_lstm.add(embedding_layer)\n",
    "model_lstm.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Bidirectional(LSTM(128)))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(len(np.unique(y_train)),activation=\"softmax\"))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model_lstm.summary())\n",
    "plot_model(model_lstm, to_file='model_lstm_filipino.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/15\n",
      "1780/1780 [==============================] - 1257s 706ms/step - loss: 1.4769 - acc: 0.6185 - val_loss: 1.2821 - val_acc: 0.8629\n",
      "Epoch 2/15\n",
      "1780/1780 [==============================] - 1154s 648ms/step - loss: 0.9883 - acc: 0.8747 - val_loss: 0.5649 - val_acc: 0.9169\n",
      "Epoch 3/15\n",
      "1780/1780 [==============================] - 1137s 639ms/step - loss: 0.3605 - acc: 0.9309 - val_loss: 0.3112 - val_acc: 0.9079\n",
      "Epoch 4/15\n",
      "1780/1780 [==============================] - 1167s 655ms/step - loss: 0.2368 - acc: 0.9399 - val_loss: 0.2172 - val_acc: 0.9348\n",
      "Epoch 5/15\n",
      "1780/1780 [==============================] - 1211s 680ms/step - loss: 0.2044 - acc: 0.9472 - val_loss: 0.1634 - val_acc: 0.9483\n",
      "Epoch 6/15\n",
      "1780/1780 [==============================] - 1206s 677ms/step - loss: 0.1711 - acc: 0.9522 - val_loss: 0.1576 - val_acc: 0.9506\n",
      "Epoch 7/15\n",
      "1780/1780 [==============================] - 1198s 673ms/step - loss: 0.1372 - acc: 0.9629 - val_loss: 0.1542 - val_acc: 0.9506\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0.003)\n",
    "history_lstm=model_lstm.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=50,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph_LSTM_Filipino')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 71s 160ms/step\n",
      "['loss', 'acc']\n",
      "[0.15417643115761576, 0.9505617992261822]\n"
     ]
    }
   ],
   "source": [
    "#model_lstm.save(\"LSTM.h5\")\n",
    "scores = model_lstm.evaluate(test_sentences_X,y_test)\n",
    "print(model_lstm.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 81s 183ms/step\n",
      "['loss', 'acc']\n",
      "[0.15417643115761576, 0.9505617992261822]\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "model_lstm.save_weights('lstm_weights_Filipino.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('lstm_architecture_Filipino.json', 'w') as f:\n",
    "    f.write(model_lstm.to_json())\n",
    "    \n",
    "from keras.models import model_from_json\n",
    "del model_lstm\n",
    "# Model reconstruction from JSON file\n",
    "with open('lstm_architecture_Filipino.json', 'r') as f:\n",
    "    model_lstm = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model_lstm.load_weights('lstm_weights_Filipino.h5')\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "scores = model_lstm.evaluate(test_sentences_X,y_test)\n",
    "print(model_lstm.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history_lstm.history['acc'])\n",
    "plt.plot(history_lstm.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_lstm_fil.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_lstm_fil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Long-Short  Term Memory with Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1749)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1749, 300)    7512900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1749, 256)    439296      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1749, 256)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1749, 256)    394240      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 256, 1749)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256, 1749)    3060750     permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 1749, 256)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1749, 256)    0           bidirectional_2[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1749, 256)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 447744)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            2238725     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,645,911\n",
      "Trainable params: 6,133,011\n",
      "Non-trainable params: 7,512,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge,concatenate,add,dot,multiply, Dense,CuDNNLSTM, LSTM, InputLayer, Bidirectional, Embedding, Activation,Dropout\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import plot_model\n",
    "\n",
    "inputs = Input(shape=(MAX_LENGTH,))\n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "attention_mul = embedding_layer(inputs)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul = attention_3d_block(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(len(np.unique(y_train)),activation=\"softmax\")(attention_mul)\n",
    "model = Model(inputs=[inputs], outputs=output)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_attention_Filipino.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/15\n",
      "1780/1780 [==============================] - 1782s 1s/step - loss: 1.6072 - acc: 0.5067 - val_loss: 1.5977 - val_acc: 0.5438\n",
      "Epoch 2/15\n",
      "1780/1780 [==============================] - 1769s 994ms/step - loss: 1.4248 - acc: 0.4803 - val_loss: 1.0760 - val_acc: 0.5910\n",
      "Epoch 3/15\n",
      "1780/1780 [==============================] - 1771s 995ms/step - loss: 0.9439 - acc: 0.6073 - val_loss: 0.8150 - val_acc: 0.6854\n",
      "Epoch 4/15\n",
      "1780/1780 [==============================] - 1767s 993ms/step - loss: 0.7496 - acc: 0.7090 - val_loss: 0.6944 - val_acc: 0.7146\n",
      "Epoch 5/15\n",
      "1780/1780 [==============================] - 1757s 987ms/step - loss: 0.6049 - acc: 0.7534 - val_loss: 0.5415 - val_acc: 0.7753\n",
      "Epoch 6/15\n",
      "1780/1780 [==============================] - 1747s 981ms/step - loss: 0.5039 - acc: 0.8084 - val_loss: 0.4407 - val_acc: 0.8517\n",
      "Epoch 7/15\n",
      "1780/1780 [==============================] - 1765s 992ms/step - loss: 0.3873 - acc: 0.9034 - val_loss: 0.3863 - val_acc: 0.9056\n",
      "Epoch 8/15\n",
      "1780/1780 [==============================] - 1744s 980ms/step - loss: 0.3369 - acc: 0.9219 - val_loss: 0.3272 - val_acc: 0.9348\n",
      "Epoch 9/15\n",
      "1780/1780 [==============================] - 1774s 997ms/step - loss: 0.2777 - acc: 0.9461 - val_loss: 0.2709 - val_acc: 0.9326\n",
      "Epoch 10/15\n",
      "1780/1780 [==============================] - 1765s 991ms/step - loss: 0.2148 - acc: 0.9534 - val_loss: 0.2371 - val_acc: 0.9416\n",
      "Epoch 11/15\n",
      "1780/1780 [==============================] - 1747s 981ms/step - loss: 0.3552 - acc: 0.9090 - val_loss: 0.2297 - val_acc: 0.9258\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1)\n",
    "history=model.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=32,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph_Filipino')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 93s 208ms/step\n",
      "['loss', 'acc']\n",
      "[0.22971181260066087, 0.9258426968970995]\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "model.save_weights('attention_weights_Filipino.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('attention_architecture_Filipino.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "from keras.models import model_from_json\n",
    "del model\n",
    "# Model reconstruction from JSON file\n",
    "with open('attention_architecture_Filipino.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('attention_weights_Filipino.h5')\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "scores = model.evaluate(test_sentences_X,y_test)\n",
    "print(model.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_attention_fil.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_attention_fil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'is', 'in', 'intensive', 'care', 'after', 'suffering', 'a', 'stroke', '.', 'known', 'for', 'movies', 'including', 'Boyz', 'N', 'The', 'Hood', 'and', '2', 'Fast', '2', 'Furious', ',', 'became', 'unwell', 'on', 'Wednesday', ',', 'his', 'family', 'said', '.', 'Following', 'news', 'of', 'the', 'stroke', ',', 'friends', 'and', 'colleagues', 'sent', 'their', 'best', 'wishes', '.', 'Rapper', 'Snoop', 'Dogg', 'shared', 'a', 'picture', 'of', 'the', 'pair', 'together', 'on', 'Instagram', ',', 'writing', ':', 'Pray', '4', 'my', 'brother', '.']\n",
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'intensive', 'care', 'suffering', 'stroke', 'known', 'movies', 'including', 'Boyz', 'Hood', 'Fast', 'Furious', 'became', 'unwell', 'Wednesday', 'family', 'said', 'Following', 'news', 'stroke', 'friends', 'colleagues', 'sent', 'best', 'wishes', 'Rapper', 'Snoop', 'Dogg', 'shared', 'picture', 'pair', 'together', 'Instagram', 'writing', 'Pray', 'brother']\n",
      "[1, 18319, 17823, 1, 3156, 12852, 17711, 18704, 3781, 4997, 16918, 1, 4809, 5163, 21423, 13750, 3787, 7131, 2788, 18581, 10005, 15050, 18704, 13666, 13235, 10162, 22430, 20638, 6068, 13899, 10982, 12850, 16746, 2396, 21272, 1, 636, 918, 17689]\n",
      "[[    1 18319 17823 ...     0     0     0]]\n",
      "[[0.00194192 0.8597092  0.01367367 0.09096657 0.03370867]]\n"
     ]
    }
   ],
   "source": [
    "from attention_utils import get_activations, get_data_recurrent\n",
    "attention_vectors = []\n",
    "\n",
    "input_text=\"Oscar-nominated director John Singleton is in intensive care after suffering a stroke. known for movies including Boyz N The Hood and 2 Fast 2 Furious, became unwell on Wednesday, his family said. Following news of the stroke, friends and colleagues sent their best wishes. Rapper Snoop Dogg shared a picture of the pair together on Instagram, writing: Pray 4 my brother. \" \n",
    "input_text=input_text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "input_text=word_tokenize(input_text)\n",
    "print(input_text)\n",
    "input_text_cleaned=[]\n",
    "for w in input_text:\n",
    "    if w.lower() not in stopwords.words(\"english\") and len(w)>=3:\n",
    "        input_text_cleaned.append(w)\n",
    "print(input_text_cleaned)\n",
    "input_text_index=[]\n",
    "for w in input_text_cleaned:\n",
    "    try:\n",
    "        input_text_index.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        input_text_index.append(word2index['-OOV-'])\n",
    "print(input_text_index)\n",
    "input_text_pad=pad_sequences([input_text_index], maxlen=MAX_LENGTH, padding='post')\n",
    "print(input_text_pad)\n",
    "\n",
    "print(model.predict(input_text_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 50, 30, 187, 55, 22, 21, 33, 11, 35]\n",
      "{'colleagues': 0.00027969663, 'Dogg': 0.00029982178, 'stroke': 0.0006032483, 'news': 0.0033030058, 'pair': 0.1252957, 'Boyz': 0.3683312, 'Instagram': 0.43804774}\n"
     ]
    }
   ],
   "source": [
    "attention_vector = np.mean(get_activations(model,input_text_pad,print_shape_only=True,layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "\n",
    "words_index=sorted(range(len(attention_vector)), key=lambda i: attention_vector[i])[-10:]\n",
    "print(words_index)\n",
    "top_words=dict()\n",
    "for x in words_index:\n",
    "    if x<len(input_text_cleaned):\n",
    "        top_words[input_text_cleaned[x]]=attention_vector[x]\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
