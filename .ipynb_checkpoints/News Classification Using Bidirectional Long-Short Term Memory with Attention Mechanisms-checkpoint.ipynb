{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing, Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = dirName+\"/\"+entry\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the data into a pandas dataframe </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dirName = 'data';\n",
    "    \n",
    "# Get the list of all files in directory tree at given path\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "    \n",
    "\n",
    "df=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"])\n",
    "# Print the files    \n",
    "\n",
    "for elem in listOfFiles:\n",
    "    file1 = open(elem,\"r\") \n",
    "    sampleNews=file1.read().split(\"\\n\")\n",
    "    newsDesc=\"\"\n",
    "    for strline in sampleNews[1:len(sampleNews)]:\n",
    "        newsDesc+=strline\n",
    "    \"\"\"\n",
    "    print(\"Category: \"+elem.split(\"/\")[1])\n",
    "    print(\"Title:\"+sampleNews[0])\n",
    "    print(\"Description:\"+newsDesc[0:50])\n",
    "    \"\"\"\n",
    "    dfsample=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"],data=[[sampleNews[0],newsDesc,elem.split(\"/\")[1]]])\n",
    "    df=df.append(dfsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df=df.drop(list(df)[0], axis=1)\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convert the Labels to Integers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=df[\"Category\"].unique().tolist()\n",
    "cat_dict=dict(zip(categories, range(0,len((categories)))))\n",
    "print(cat_dict)\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(df[df[\"Category\"]==x])))\n",
    "df[\"Category\"]=df['Category'].map(cat_dict, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing Special Characters </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df=df.dropna()\n",
    "df[\"Description\"].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 80-20 Train-Test Split </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df[\"Description\"], df[\"Category\"], test_size=0.20, random_state=42)\n",
    "\n",
    "train_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_train[y_train==cat_dict[x]])))\n",
    "    train_sum+=len(y_train[y_train==cat_dict[x]])\n",
    "print(\"Total Training Data: \"+str(train_sum)+\"\\n\")\n",
    "\n",
    "test_sum=0\n",
    "for x in categories:\n",
    "    print(x+\": \"+str(len(y_test[y_test==cat_dict[x]])))\n",
    "    test_sum+=len(y_test[y_test==cat_dict[x]])\n",
    "print(\"Total Test Data: \"+str(test_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_notcleaned=[word_tokenize(i) for i in X_train]\n",
    "y_train=y_train.values\n",
    "X_test_notcleaned=[word_tokenize(i) for i in X_test]\n",
    "y_test=y_test.values\n",
    "\n",
    "print(X_train_notcleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing of Stop Words and Words with length <3 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(s):\n",
    "    s_cleaned=[]\n",
    "    for w in s:\n",
    "        if w.lower() not in stopwords.words(\"english\") and len(w)>=3:\n",
    "            s_cleaned.append(w)\n",
    " \n",
    "    return s_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import multiprocessing as mp\n",
    "import data_clean\n",
    "\n",
    "\n",
    "X_train,X_test=[],[]\n",
    "\n",
    "pool = mp.Pool(processes=4)\n",
    "X_train = pool.map(data_clean.remove_stop_words, X_train_notcleaned)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pool.map(data_clean.remove_stop_words, X_test_notcleaned)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4> <p> Creating the Vocabulary and word2index </p> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set([])\n",
    "for s in X_train:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Training and Test Data before fitting into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing the GloVe word embeddings</h4>\n",
    "\n",
    "<p> You can download the word embeddings at https://nlp.stanford.edu/projects/glove/ </p>\n",
    "<br><b>Note:</b> Choose the 6B and 300D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt',encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert all words to lowercase then to integers, then pad the sentences </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X, test_sentences_X = [], []\n",
    "\n",
    "EMB_DIM=300\n",
    "num_words=len(word2index)+1\n",
    "print(\"Number of Words:\"+str(num_words))\n",
    "\n",
    "for s in X_train:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in X_test:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "#y_train=to_categorical(y_train)\n",
    "#y_test=to_categorical(y_test)\n",
    "print(train_sentences_X[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <h3>Creation of Pre-Trained Word Embeddings to be used for the embedding layer </h3> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((num_words,EMB_DIM))\n",
    "#print(word2index)\n",
    "for word,i in word2index.items():\n",
    "    if i>num_words:\n",
    "        continue\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTMs with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper of <i> Attention is All You Need </i>, we have the follow equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Attention(Q,K,V) &  = Softmax(\\frac{QK^T}{\\sqrt d_k}) V \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "TIME_STEPS=MAX_LENGTH\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = EMB_DIM\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard callback for visualizations of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./Graph', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Long-Short Term Memory without the Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1749, 300)         7512900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 7,953,481\n",
      "Trainable params: 440,581\n",
      "Non-trainable params: 7,512,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, CuDNNLSTM,LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "model_lstm=Sequential() \n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "#embedding_layer=Embedding(num_words, 300,mask_zero=True)\n",
    "inputs=InputLayer(input_shape=(MAX_LENGTH, ))\n",
    "model_lstm.add(inputs)\n",
    "model_lstm.add(embedding_layer)\n",
    "model_lstm.add(Bidirectional(LSTM(128)))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(len(np.unique(y_train)),activation=\"softmax\"))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model_lstm.summary())\n",
    "plot_model(model_lstm, to_file='model_lstm.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/15\n",
      "1780/1780 [==============================] - 27s 15ms/step - loss: 1.5451 - acc: 0.3393 - val_loss: 1.4347 - val_acc: 0.6315\n",
      "Epoch 2/15\n",
      "1780/1780 [==============================] - 24s 14ms/step - loss: 1.3319 - acc: 0.6927 - val_loss: 1.2043 - val_acc: 0.8270\n",
      "Epoch 3/15\n",
      "1780/1780 [==============================] - 23s 13ms/step - loss: 1.0472 - acc: 0.8427 - val_loss: 0.8411 - val_acc: 0.8944\n",
      "Epoch 4/15\n",
      "1780/1780 [==============================] - 23s 13ms/step - loss: 0.6405 - acc: 0.8949 - val_loss: 0.4528 - val_acc: 0.9258\n",
      "Epoch 5/15\n",
      "1780/1780 [==============================] - 25s 14ms/step - loss: 0.3473 - acc: 0.9360 - val_loss: 0.2761 - val_acc: 0.9326\n",
      "Epoch 6/15\n",
      "1780/1780 [==============================] - 25s 14ms/step - loss: 0.2244 - acc: 0.9483 - val_loss: 0.2512 - val_acc: 0.9303\n",
      "Epoch 7/15\n",
      "1780/1780 [==============================] - 24s 14ms/step - loss: 0.1826 - acc: 0.9528 - val_loss: 0.2460 - val_acc: 0.9258\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1,min_delta=0.003)\n",
    "history_lstm=model_lstm.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=64,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph_LSTM')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 4s 8ms/step\n",
      "['loss', 'acc']\n",
      "[0.19736234123787183, 0.9483146070094591]\n"
     ]
    }
   ],
   "source": [
    "model_lstm.save(\"LSTM.h5\")\n",
    "scores = model_lstm.evaluate(test_sentences_X,y_test)\n",
    "print(model_lstm.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history_lstm.history['acc'])\n",
    "plt.plot(history_lstm.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_lstm.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_lstm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Long-Short  Term Memory with Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1749)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1749, 300)    7512900     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1749, 256)    439296      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1749, 256)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1749, 256)    394240      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 256, 1749)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256, 1749)    3060750     permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 1749, 256)    0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1749, 256)    0           bidirectional_3[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1749, 256)    0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 447744)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            2238725     flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,645,911\n",
      "Trainable params: 6,133,011\n",
      "Non-trainable params: 7,512,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge,concatenate,add,dot,multiply, Dense,CuDNNLSTM, LSTM, InputLayer, Bidirectional, Embedding, Activation,Dropout\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import plot_model\n",
    "\n",
    "inputs = Input(shape=(MAX_LENGTH,))\n",
    "embedding_layer=Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LENGTH,trainable=False)\n",
    "attention_mul = embedding_layer(inputs)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Bidirectional(LSTM(128,return_sequences=True,activation=\"tanh\"))(attention_mul)\n",
    "attention_mul = attention_3d_block(attention_mul)\n",
    "attention_mul=Dropout(0.5)(attention_mul)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(len(np.unique(y_train)),activation=\"softmax\")(attention_mul)\n",
    "model = Model(inputs=[inputs], outputs=output)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(0.0001),    \n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_attention.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "es_valacc = EarlyStopping(monitor='val_acc', mode='max',verbose=1,patience=2,min_delta=0.003)\n",
    "es_loss = EarlyStopping(monitor='loss', mode='min',verbose=1)\n",
    "history=model.fit(train_sentences_X, y_train, validation_data=(test_sentences_X,y_test), batch_size=32,epochs=15,callbacks=[es_loss,es_valacc,TrainValTensorBoard(write_graph=True,log_dir='./Graph')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 49s 111ms/step\n",
      "['loss', 'acc']\n",
      "[0.34802084277185164, 0.8786516844556572]\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Attention.h5\")\n",
    "scores = model.evaluate(test_sentences_X,y_test)\n",
    "print(model.metrics_names)   # acc: 99.09751977804825   \n",
    "print(scores)   # acc: 99.09751977804825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "plt.savefig(\"acc_attention.png\")\n",
    "\n",
    "plt.figure( )\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(\"loss_attention.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'is', 'in', 'intensive', 'care', 'after', 'suffering', 'a', 'stroke', '.', 'known', 'for', 'movies', 'including', 'Boyz', 'N', 'The', 'Hood', 'and', '2', 'Fast', '2', 'Furious', ',', 'became', 'unwell', 'on', 'Wednesday', ',', 'his', 'family', 'said', '.', 'Following', 'news', 'of', 'the', 'stroke', ',', 'friends', 'and', 'colleagues', 'sent', 'their', 'best', 'wishes', '.', 'Rapper', 'Snoop', 'Dogg', 'shared', 'a', 'picture', 'of', 'the', 'pair', 'together', 'on', 'Instagram', ',', 'writing', ':', 'Pray', '4', 'my', 'brother', '.']\n",
      "['Oscar-nominated', 'director', 'John', 'Singleton', 'intensive', 'care', 'suffering', 'stroke', 'known', 'movies', 'including', 'Boyz', 'Hood', 'Fast', 'Furious', 'became', 'unwell', 'Wednesday', 'family', 'said', 'Following', 'news', 'stroke', 'friends', 'colleagues', 'sent', 'best', 'wishes', 'Rapper', 'Snoop', 'Dogg', 'shared', 'picture', 'pair', 'together', 'Instagram', 'writing', 'Pray', 'brother']\n",
      "[1, 17616, 302, 1, 7417, 20583, 9708, 13571, 14460, 6609, 2637, 1, 15688, 18429, 17482, 16864, 21457, 11967, 3919, 9586, 2998, 21979, 13571, 6306, 12291, 21443, 7644, 19701, 22094, 20193, 13844, 2181, 4854, 3613, 5813, 1, 6839, 15335, 1816]\n",
      "[[    1 17616   302 ...     0     0     0]]\n",
      "[[0.01235872 0.7170878  0.07023467 0.17854196 0.02177682]]\n"
     ]
    }
   ],
   "source": [
    "from attention_utils import get_activations, get_data_recurrent\n",
    "attention_vectors = []\n",
    "\n",
    "input_text=\"Oscar-nominated director John Singleton is in intensive care after suffering a stroke. known for movies including Boyz N The Hood and 2 Fast 2 Furious, became unwell on Wednesday, his family said. Following news of the stroke, friends and colleagues sent their best wishes. Rapper Snoop Dogg shared a picture of the pair together on Instagram, writing: Pray 4 my brother. \" \n",
    "input_text=input_text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "input_text=word_tokenize(input_text)\n",
    "print(input_text)\n",
    "input_text_cleaned=[]\n",
    "for w in input_text:\n",
    "    if w.lower() not in stopwords.words(\"english\") and len(w)>=3:\n",
    "        input_text_cleaned.append(w)\n",
    "print(input_text_cleaned)\n",
    "input_text_index=[]\n",
    "for w in input_text_cleaned:\n",
    "    try:\n",
    "        input_text_index.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        input_text_index.append(word2index['-OOV-'])\n",
    "print(input_text_index)\n",
    "input_text_pad=pad_sequences([input_text_index], maxlen=MAX_LENGTH, padding='post')\n",
    "print(input_text_pad)\n",
    "\n",
    "print(model.predict(input_text_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 89, 18, 6, 1, 46, 9, 13, 12, 14]\n",
      "{'family': 0.00047280855, 'suffering': 0.00047987126, 'director': 0.0005026029, 'movies': 0.00078302884, 'Fast': 0.001060341, 'Hood': 0.39211977, 'Furious': 0.461493}\n"
     ]
    }
   ],
   "source": [
    "attention_vector = np.mean(get_activations(model,input_text_pad,print_shape_only=True,layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "\n",
    "words_index=sorted(range(len(attention_vector)), key=lambda i: attention_vector[i])[-10:]\n",
    "print(words_index)\n",
    "top_words=dict()\n",
    "for x in words_index:\n",
    "    if x<len(input_text_cleaned):\n",
    "        top_words[input_text_cleaned[x]]=attention_vector[x]\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
